<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Richard‘s Blog</title>
  <icon>https://www.gravatar.com/avatar/a426ef5f9b38de213a4373837416fea8</icon>
  <subtitle>记录敲码过程中遇到的细节和问题</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://richardrw.github.io/"/>
  <updated>2017-11-14T12:47:02.000Z</updated>
  <id>https://richardrw.github.io/</id>
  
  <author>
    <name>Richard</name>
    <email>weichang321@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>把《Flask Web开发》读系列之第二章 程序的基本概念</title>
    <link href="https://richardrw.github.io/2017/11/14/%E6%8A%8A%E3%80%8AFlask%20Web%E5%BC%80%E5%8F%91%E3%80%8B%E8%AF%BB%E7%B3%BB%E5%88%97%E4%B9%8B%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E7%A8%8B%E5%BA%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>https://richardrw.github.io/2017/11/14/把《Flask Web开发》读系列之第二章 程序的基本概念/</id>
    <published>2017-11-14T12:45:31.000Z</published>
    <updated>2017-11-14T12:47:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2-1-初始化"><a href="#2-1-初始化" class="headerlink" title="2.1 初始化"></a>2.1 初始化</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from flask import Flask</div><div class="line"></div><div class="line">app = Flask(__name__)</div></pre></td></tr></table></figure><p>Flask实例化只有一个必须指定的参数，即程序主模块或包的名字（也即<code>__name__</code>）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"># 2.2 路由和视图函数</div><div class="line">1. **路由**：处理URL和函数之间的关系（URL到Python函数的映射关系）的程序。</div><div class="line">```可通过Flask实例提供的`app.route()`装饰器把函数注册为路由。</div></pre></td></tr></table></figure></p><ol><li><strong>视图函数</strong>：像<code>index()</code>这样的函数。</li></ol><h1 id="2-4-一个完整的程序"><a href="#2-4-一个完整的程序" class="headerlink" title="2.4 一个完整的程序"></a>2.4 一个完整的程序</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">from flask import Flask</div><div class="line"></div><div class="line">app = Flask(__name__)</div><div class="line"></div><div class="line">@app.route(&apos;/&apos;)</div><div class="line">def index():</div><div class="line">    return &apos;&lt;h1&gt;Hello World!&lt;/h1&gt;</div><div class="line"></div><div class="line">@app.route(&apos;/user/&lt;name&gt;&apos;)      # &lt; &gt;尖括号部门是动态url部分</div><div class="line">def user(name):                 # Flask会将动态部分（也即&lt; &gt;部分）作为参数传入视图函数中</div><div class="line">    return &apos;&lt;h1&gt;Hello &#123;&#125;!&apos;.format(name)</div><div class="line">    </div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    app.run(debug=True)</div></pre></td></tr></table></figure><h1 id="2-5-请求-响应"><a href="#2-5-请求-响应" class="headerlink" title="2.5 请求-响应"></a>2.5 请求-响应</h1><h2 id="2-5-1-程序与请求上下文"><a href="#2-5-1-程序与请求上下文" class="headerlink" title="2.5.1 程序与请求上下文"></a>2.5.1 程序与请求上下文</h2><ol><li><code>程序上下文</code>：</li><li><code>请求上下文</code>:</li></ol><h2 id="2-5-2-请求调度"><a href="#2-5-2-请求调度" class="headerlink" title="2.5.2 请求调度"></a>2.5.2 请求调度</h2><p>Flask使用<code>app.route()</code>装饰器或者非装饰器形式的<code>app.add_url_rule()</code>生成<strong>URL和视图函数之间的映射</strong></p><h2 id="2-5-3-请求钩子"><a href="#2-5-3-请求钩子" class="headerlink" title="2.5.3 请求钩子"></a>2.5.3 请求钩子</h2><p><strong>应用场景：</strong>在请求开始时，创建数据库链接或认证用户。（即在处理请求<strong>之前</strong>或<strong>之后</strong>，执行某些代码）<br>Flask提供以下4种钩子：</p><ol><li><code>before_first_request</code>：注册一个函数，<strong>在处理第一个请求之前运行</strong></li><li><code>before_request</code>：注册一个函数，<strong>在每次请求之前运行</strong></li><li><code>after_request</code>：注册一个函数，<strong>如果没有未处理的异常抛出，在每次请求之后运行</strong></li><li><code>teardown_request</code>：注册一个函数，<strong>即使有未处理的异常抛出，也在每次请求之后运行</strong></li></ol><h2 id="2-5-4-响应"><a href="#2-5-4-响应" class="headerlink" title="2.5.4 响应"></a>2.5.4 响应</h2><p><strong>视图函数返回值</strong>可以接受3个参数：响应文本，状态码，一个由header组成的字典。<br><code>make_response()</code>函数可以接受3个参数（和视图函数返回值一样），并返回一个<code>response</code>对象，此时我们可以在<code>response</code>对象上调用各种方法，进一步设置响应。<br>如获得一个<code>response</code>对象，然后设置<code>cookie</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">from flask import Flask</div><div class="line">from flask import make_response</div><div class="line"></div><div class="line">app = Flask(__name__)</div><div class="line"></div><div class="line">@app.route(&apos;/&apos;)</div><div class="line">def index():</div><div class="line">    response = make_response(&apos;&lt;h1&gt;This document carries a cookie!&lt;/h1&gt;&apos;)</div><div class="line">    response.set_cookie(&apos;answer&apos;, &apos;42&apos;)</div><div class="line">    return response</div></pre></td></tr></table></figure></p><p><code>redirect()</code>函数用于生成<strong>重定向</strong>（一种特殊的响应）。</p><p><code>abort</code>函数用于生产另一种特殊响应，用于处理错误。<br>如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">from flask import Flask</div><div class="line">from flask import redirect</div><div class="line">from flask import abort</div><div class="line"></div><div class="line">app = Flask(__name__)</div><div class="line"></div><div class="line">@app.route(&apos;/&apos;)</div><div class="line">def index():</div><div class="line">    return redirect(&apos;http://www.example.com&apos;)</div><div class="line">    </div><div class="line">@app.route(&apos;/user/&lt;id&gt;&apos;)</div><div class="line">def get_user(id):</div><div class="line">    user = load_user(id)        # load_user()用于读取id，不展开写</div><div class="line">    if not user:</div><div class="line">        abort(404)        # 如果id不存在，返回状态吗404</div><div class="line">    return &apos;&lt;h1&gt;Hello &#123;&#125;&lt;/h1&gt;&apos;.format(id)</div></pre></td></tr></table></figure></p><h1 id="2-6-Flask扩展"><a href="#2-6-Flask扩展" class="headerlink" title="2.6 Flask扩展"></a>2.6 Flask扩展</h1><p>可使用<code>flask-script</code>扩展为Flask程序添加一个命令行解析器，那么就可以自定义很多参数，如监听的host、port等。<br>实现方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from flask.ext.script import Manager</div><div class="line"># ...</div><div class="line"></div><div class="line">manager = Manager(app)</div><div class="line"></div><div class="line">if __name__ = &apos;__main__&apos;:</div><div class="line">    manager.run()</div></pre></td></tr></table></figure></p><p>把Flask实例<code>app</code>作为参数传给<code>Manager()</code>，初始化实例。<br><strong>注意</strong>：Flask扩展都在<code>flask.ext</code>命名空间下，而Python3.5之后都直接使用<code>from flask-xxx import xxx</code>来导入。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;2-1-初始化&quot;&gt;&lt;a href=&quot;#2-1-初始化&quot; class=&quot;headerlink&quot; title=&quot;2.1 初始化&quot;&gt;&lt;/a&gt;2.1 初始化&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>把《Flask Web开发》读薄系列之第一章 创建虚拟环境</title>
    <link href="https://richardrw.github.io/2017/11/14/%E6%8A%8A%E3%80%8AFlask%20Web%E5%BC%80%E5%8F%91%E3%80%8B%E8%AF%BB%E8%96%84%E7%B3%BB%E5%88%97%E4%B9%8B%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"/>
    <id>https://richardrw.github.io/2017/11/14/把《Flask Web开发》读薄系列之第一章 创建虚拟环境/</id>
    <published>2017-11-14T12:43:30.000Z</published>
    <updated>2017-11-14T12:44:59.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li>切换到目标文件夹目录</li><li>执行<code>virtualenv venv</code>命令，其中<code>venv</code>为虚拟环境名称</li><li>执行<code>source venv/bin/activate</code>激活虚拟环境</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;切换到目标文件夹目录&lt;/li&gt;
&lt;li&gt;执行&lt;code&gt;virtualenv venv&lt;/code&gt;命令，其中&lt;code&gt;venv&lt;/code&gt;为虚拟环境名称&lt;/li&gt;
&lt;li&gt;执行&lt;code&gt;source venv/bin/activate&lt;/code&gt;激活虚
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>把《Flask Web开发》读薄系列之序言</title>
    <link href="https://richardrw.github.io/2017/11/14/%E6%8A%8A%E3%80%8AFlask%20Web%E5%BC%80%E5%8F%91%E3%80%8B%E8%AF%BB%E8%96%84%E7%B3%BB%E5%88%97%E4%B9%8B%E5%BA%8F%E8%A8%80/"/>
    <id>https://richardrw.github.io/2017/11/14/把《Flask Web开发》读薄系列之序言/</id>
    <published>2017-11-14T12:20:58.000Z</published>
    <updated>2017-11-14T12:38:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="读者的疑惑"><a href="#读者的疑惑" class="headerlink" title="读者的疑惑"></a>读者的疑惑</h1><p>《把<flask web开发——基于python的web应用开发实战="">读薄系列》主要是对《Flask Web开发》一书的概括，或许你读起来会吐槽说“卧靠，这不就是书中的内容吗，直接把书中的内容搬过来就可以成一个系列了，真牛逼“。对此，我只能说读书笔记大概就是这样吧。你对其中的内容懂了，就会觉得很容易，觉得很没必要做这样的一种记录，完全是照搬书中内容，没有什么实质东西。但是，该系列也并不完全是书中内容，它还包括作者对一些该概念的理解、类比，以及代码实现过程中可能遇到的、应该注意的坑等。如：把第五章数据库中的5.11 使用Flask-Migrate实现数据库迁移，跟Git版本控制联系起来，结合理解。如：第六章电子邮件中通过QQ邮箱服务器发送邮件应该注意的坑，以及拓展对<code>**kwargs</code>的理解和实际应用情景。总的来说，该系列主要起到的作用如下：</flask></p><ul><li>起到思维导图作用。当你初学完或者之前学过但是没有实际应用过，那么该系列可以帮助你勾起记忆，尽快掌握Flask的一些基本知识。另外也能让你根据思维导图你查漏补缺。</li><li>加入一些类比，加强理解书中概念。如前面说到的Flask-Migrate与Git的类比。书中的一些类比能否帮助你，视人而已，因为每个人对概念的理解都不同，就像不是所有读者都了解Git一样。</li><li>拓展对Python一些概念的理解。亦如前面说到的对<code>**kwargs</code>的理解。</li></ul><h1 id="最后说两句"><a href="#最后说两句" class="headerlink" title="最后说两句"></a>最后说两句</h1><p>可能是翻译的原因，书中偶尔会出现的一些句子或内容读起来很费劲，很难理解，对此，作者也尽量以平实的语言概括该系列，甚至是以提炼出“存在问题—解决思路—解决方法“或者问答的形式来概括书中内容，加深理解。如第四章4.5 重定向和用户会话，作者根据书中内容提炼出“存在问题—解决思路—解决方法”来解释为什么使用重定向和用户会话。</p><p>另外，为了更准确地表达意思，该系列中的一些章节的名字可能与原书的章节名字不同，但这并不要紧，意思对了就行。</p><p>最后还是要感谢一下@路人甲，他在小密圈对圈友发布的读书笔记奖励活动，激励大家读书长知识。（虽然该系列在发发布活动前就已经在写哈哈，但也起到了很大的激励作用，营造出了一种学习气氛）</p><h1 id="联系作者"><a href="#联系作者" class="headerlink" title="联系作者"></a>联系作者</h1><p>如果你对《Flask Web开发——基于Python的Web应用开发实战》书中的内容有一些很好的个人理解（如用类比来理解）等，可以把你的理解整理好发送到作者邮箱：weichang321@gmail.com。我将会筛选一些好的内容加入到该系列中，并署上你的名字。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;读者的疑惑&quot;&gt;&lt;a href=&quot;#读者的疑惑&quot; class=&quot;headerlink&quot; title=&quot;读者的疑惑&quot;&gt;&lt;/a&gt;读者的疑惑&lt;/h1&gt;&lt;p&gt;《把&lt;flask web开发——基于python的web应用开发实战=&quot;&quot;&gt;读薄系列》主要是对《Flask Web开
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）</title>
    <link href="https://richardrw.github.io/2017/10/20/%E6%9E%84%E5%BB%BA%E7%88%AC%E5%8F%96%E5%A4%A7%E4%BC%97%E7%82%B9%E8%AF%84%E7%BE%8E%E9%A3%9F%E6%95%B0%E6%8D%AE%E7%9A%84%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%88%AC%E8%99%AB%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/</id>
    <published>2017-10-20T14:07:02.000Z</published>
    <updated>2017-11-04T14:29:19.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="构建爬取大众点评美食数据的多进程爬虫（一）"><a href="#构建爬取大众点评美食数据的多进程爬虫（一）" class="headerlink" title="构建爬取大众点评美食数据的多进程爬虫（一）"></a>构建爬取大众点评美食数据的多进程爬虫（一）</h1><h2 id="一、要抓取哪些数据"><a href="#一、要抓取哪些数据" class="headerlink" title="一、要抓取哪些数据"></a>一、要抓取哪些数据</h2><p>以东莞美食为例<a href="http://www.dianping.com/search/category/219/10/g0r0" target="_blank" rel="external">大众点评-美食-东莞</a>，可以看到，在美食页面，我们想要爬取的信息有：</p><ol><li>商户标题</li><li>星级</li><li>评论数</li><li>人均价格</li><li>各项评分</li><li>菜系</li><li>地址</li></ol><h2 id="二、分析页面"><a href="#二、分析页面" class="headerlink" title="二、分析页面"></a>二、分析页面</h2><h3 id="（一）多观察页面"><a href="#（一）多观察页面" class="headerlink" title="（一）多观察页面"></a>（一）多观察页面</h3><p>通过点击<a href="http://www.dianping.com/search/category/219/10/g112" target="_blank" rel="external">小吃快餐</a>，我们看到小吃快餐下一共有50个页面。再点击<a href="http://www.dianping.com/search/category/219/10/g117" target="_blank" rel="external">面包甜点</a>，发现也只有50个页面。接下来我们看看<a href="http://www.dianping.com/search/category/219/10/g117r434" target="_blank" rel="external">面包甜点-东城区</a>，发现最多也只有50个页面。如此类推，我们大概可以确定，某菜系-某区域最多只有50个页面，由此基本可以判断，这应该是大众点评做大反爬虫措施。对此，我们在稍后的爬虫设计中，不能简单地从某一菜系下爬取所有页面或者从某一区域下爬取所有页面，因为单独选定菜系或者区域，服务器最多只返回50个页面给你。<strong> 因此我们需要将菜系-区域组合起来爬取，尽可能多的爬取商户信息。 </strong><br>通过观察，我们发现在一个网页中，如：<a href="http://www.dianping.com/search/category/219/10/g117r434" target="_blank" rel="external">http://www.dianping.com/search/category/219/10/g117r434</a><br><code>g117</code>代表菜系，<code>r434</code>代表区域，因此我们可以获取所有的菜系链接，接着在菜系链接的基础上获取菜系-区域链接，这就相当于在用浏览器浏览时，先选定了某一菜系，再选定某一区域。</p><h3 id="（二）分析页面结构"><a href="#（二）分析页面结构" class="headerlink" title="（二）分析页面结构"></a>（二）分析页面结构</h3><p>这里就是要通过定位来选定我们需要的元素。如一级菜系，它所处的位置是<code>id=&quot;classfy</code>的<code>div</code>标签下的<code>a</code>标签中。<br>商户标题，它所处的位置是<code>class=&quot;tit&quot;</code>的<code>div</code>标签下的第一个<code>a</code>标签中。通过一个个地查看，我们可以得出我们要爬取的7个商户信息的位置，方便我们后续设计爬虫时进行定位。</p><h2 id="三、设计爬虫"><a href="#三、设计爬虫" class="headerlink" title="三、设计爬虫"></a>三、设计爬虫</h2><h3 id="（一）爬虫思路"><a href="#（一）爬虫思路" class="headerlink" title="（一）爬虫思路"></a>（一）爬虫思路</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx1.sinaimg.cn/large/006XG4i7gy1fkpn3b1n9rj30fc0gpdgi.jpg" alt="爬虫流程图" title="">                </div>                <div class="image-caption">爬虫流程图</div>            </figure><ol><li>从<code>start_url</code>开始，爬取所有一级菜系链接，得到<code>tag1_url</code>，存入数据库。</li><li>从数据库中读取<code>tag1_url</code>，爬取所有二级菜系链接，得到<code>tag2_url</code>，存入数据库。</li><li>从数据库中读取<code>tag2_url</code>，爬取所有一级区域链接，得到<code>addr1_url</code>，存入数据库。</li><li>从数据库中读取<code>addr1_url</code>，爬取所有二级区域链接，得到<code>addr2_url</code>，存入数据库。</li><li>从数据库中读取<code>addr2_url</code>，爬取所有商户信息，得到<code>dpshop_msg</code>，存入数据库。<br>1-4目的都是一样，为了获取最终要爬取的页面链接，5就是为了实际爬取上述7个商户信息，所以我们把1-4写到<code>cate_parsing.py</code>文件中，把5写到<code>shop_parsing.py</code>文件中。另外，为了应对反爬虫，我们将用于伪装的<code>User-Agent</code>和<code>代理IP</code>等一些爬虫参数写到<code>config.py</code>文件中。<h3 id="（二）构建代码"><a href="#（二）构建代码" class="headerlink" title="（二）构建代码"></a>（二）构建代码</h3><code>config.py</code>代码如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">#coding=utf-8</div><div class="line"></div><div class="line"></div><div class="line">#伪装浏览器</div><div class="line">USER_AGENT = [</div><div class="line">    &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)&apos;,</div><div class="line">    &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36&apos;,</div><div class="line">    &apos;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11&apos;,</div><div class="line">    &apos;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16&apos;,</div><div class="line">    &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0&apos;,</div><div class="line">    &apos;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&apos;</div><div class="line">    ]</div><div class="line"></div><div class="line"># 代理IP</div><div class="line">PROXY = [</div><div class="line">    &apos;111.13.111.184:80&apos;,</div><div class="line">    &apos;49.119.164.175:80&apos;,</div><div class="line">    &apos;61.136.163.245:3128&apos;,</div><div class="line">    &apos;116.199.2.210:80&apos;,</div><div class="line">    &apos;116.199.2.209:80&apos;,</div><div class="line">    &apos;116.199.115.79:80&apos;,</div><div class="line">    &apos;116.199.2.196:80&apos;,</div><div class="line">    &apos;121.40.199.105:80&apos;,</div><div class="line">    &apos;125.77.25.118:80&apos;,</div><div class="line">    &apos;122.228.253.55:808&apos;</div><div class="line">    ]</div><div class="line"></div><div class="line">TIMEOUT = 5</div><div class="line"></div><div class="line">LINKTIME = 3</div><div class="line"></div><div class="line">PAGE_NUM_MAX = 50</div></pre></td></tr></table></figure></li></ol><p><code>cate_parsing.py</code>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div></pre></td><td class="code"><pre><div class="line">#coding=utf-8</div><div class="line"></div><div class="line">import requests</div><div class="line">import random</div><div class="line">import pymongo</div><div class="line">import time</div><div class="line">from lxml import etree</div><div class="line">from config import USER_AGENT, PROXY, TIMEOUT, LINKTIME</div><div class="line"></div><div class="line">client = pymongo.MongoClient(&apos;localhost&apos;, 27017)</div><div class="line">dp = client[&apos;dp&apos;]</div><div class="line"></div><div class="line"></div><div class="line"># 爬取一级菜系、二级菜系、二级菜系下的一级地址</div><div class="line">class GetTagAddr(object):</div><div class="line">    headers = random.choice(USER_AGENT)</div><div class="line">    proxies = &#123;&apos;http&apos;:random.choice(PROXY)&#125;</div><div class="line">    s = requests.Session()</div><div class="line">    s.headers.update(&#123;&apos;User-Agent&apos;:headers&#125;)</div><div class="line">    linktime = LINKTIME</div><div class="line">    timeout = TIMEOUT</div><div class="line">    tag1_url_db = dp[&apos;tag1_url_db&apos;]             # 存储从start_url中成功爬取到的tag1_url</div><div class="line">    tag2_url_db = dp[&apos;tag2_url_db&apos;]                    # 存储从tag1_url中成功爬取到的tag2_url</div><div class="line">    crawly_tag1_ok = dp[&apos;crawly_tag1_ok&apos;]    # 存储爬取成功的tag1_url</div><div class="line">    addr1_url_db = dp[&apos;addr1_url_db&apos;]            # 存储从tag2_url中成功爬取到的addr1_url</div><div class="line">    crawly_tag2_ok = dp[&apos;crawly_tag2_ok&apos;]    # 存储爬取成功的tag2_url</div><div class="line">    addr2_url_db = dp[&apos;addr2_url_db&apos;]            # 存储从addr1_url中成功爬取到的addr2_url</div><div class="line">    crawly_addr1_ok = dp[&apos;crawly_addr1_ok&apos;]  # 存储爬取成功的addr1_url</div><div class="line"></div><div class="line"></div><div class="line">    def get_tag1_from(self, start_url):</div><div class="line">        try:</div><div class="line">            r = self.s.get(start_url, proxies=self.proxies, timeout=self.timeout)</div><div class="line">            tree = etree.HTML(r.text)</div><div class="line">            tag1_items = tree.xpath(&apos;//div[@id=&quot;classfy&quot;]/a&apos;)</div><div class="line">            for i in tag1_items:</div><div class="line">                tag1 = i.getchildren()[0].text</div><div class="line">                url = i.attrib[&apos;href&apos;]</div><div class="line">                tag1_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;url&apos;: url, &apos;status&apos;: &apos;ok&apos;&#125;</div><div class="line">                self.tag1_url_db.insert_one(tag1_url_msg)</div><div class="line">            self.linktime = 3    # 重置linktime</div><div class="line">            time.sleep(1)</div><div class="line">        except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):</div><div class="line">            if self.linktime &gt; 0:</div><div class="line">                print(&apos;爬取失败，现在重新链接&apos;)</div><div class="line">                self.get_tag1_from(start_url)</div><div class="line">                self.linktime -= 1</div><div class="line">            else:</div><div class="line">                print(&apos;&#123;&#125;爬取失败&apos;.format(start_url))</div><div class="line"></div><div class="line"></div><div class="line">    def get_tag2_from(self, tag1_url):</div><div class="line">        tag1_url_msg = self.tag1_url_db.find_one(&#123;&apos;url&apos;: tag1_url&#125;)</div><div class="line">        tag1 = tag1_url_msg[&apos;tag1&apos;]</div><div class="line">        try:</div><div class="line">            r = self.s.get(tag1_url, proxies=self.proxies, timeout=self.timeout)</div><div class="line">            tree = etree.HTML(r.text)</div><div class="line">            tag2_items = tree.xpath(&apos;//div[@id=&quot;classfy-sub&quot;]/a&apos;)</div><div class="line">            if len(tag2_items) == 0:</div><div class="line">                tag2 = &apos;no_sub&apos;</div><div class="line">                tag2_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;url&apos;: tag1_url, &apos;status&apos;: &apos;tag1_not_sub&apos;&#125;</div><div class="line">                self.tag2_url_db.insert_one(tag2_url_msg)</div><div class="line">            else:</div><div class="line">                for i in tag2_items:</div><div class="line">                    tag2 = i.getchildren()[0].text</div><div class="line">                    url = i.attrib[&apos;href&apos;]</div><div class="line">                    tag2_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;url&apos;: url, &apos;status&apos;: &apos;ok&apos;&#125;</div><div class="line">                    self.tag2_url_db.insert_one(tag2_url_msg)</div><div class="line">            self.crawly_tag1_ok.insert_one(&#123;&apos;url&apos;: tag1_url&#125;)</div><div class="line">            self.linktime = 3    # 重置linktime</div><div class="line">            time.sleep(1)</div><div class="line">        except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):</div><div class="line">            if self.linktime &gt; 0:</div><div class="line">                print(&apos;爬取失败，现在重新链接&apos;)</div><div class="line">                self.get_tag2_from(tag1_url)</div><div class="line">                self.linktime -= 1</div><div class="line">            else:</div><div class="line">                print(&apos;&#123;&#125;爬取失败&apos;.format(tag1_url))</div><div class="line"></div><div class="line"></div><div class="line">    def get_addr1_from(self, tag2_url):</div><div class="line">        tag2_url_msg = self.tag2_url_db.find_one(&#123;&apos;url&apos;: tag2_url&#125;)</div><div class="line">        tag1 = tag2_url_msg[&apos;tag1&apos;]</div><div class="line">        tag2 = tag2_url_msg[&apos;tag2&apos;]</div><div class="line">        try:</div><div class="line">            r = self.s.get(tag2_url, proxies=self.proxies, timeout=self.timeout)</div><div class="line">            tree = etree.HTML(r.text)</div><div class="line">            addr1_items = tree.xpath(&apos;//div[@id=&quot;region-nav&quot;]/a&apos;)</div><div class="line">            for i in addr1_items:</div><div class="line">                addr1 = i.getchildren()[0].text</div><div class="line">                url = i.attrib[&apos;href&apos;]</div><div class="line">                addr1_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;: addr1, &apos;url&apos;: url&#125;</div><div class="line">                self.addr1_url_db.insert_one(addr1_url_msg)</div><div class="line">            self.crawly_tag2_ok.insert_one(&#123;&apos;url&apos;: tag2_url&#125;)</div><div class="line">            self.linktime = 3  # 重置linktime</div><div class="line">            time.sleep(1)</div><div class="line">        except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout,</div><div class="line">               requests.exceptions.ConnectionError):</div><div class="line">            if self.linktime &gt; 0:</div><div class="line">                print(&apos;爬取失败，现在重新链接&apos;)</div><div class="line">                self.get_addr1_from(tag2_url)</div><div class="line">                self.linktime -= 1</div><div class="line">            else:</div><div class="line">                print(&apos;&#123;&#125;爬取失败&apos;.format(tag2_url))</div><div class="line"></div><div class="line"></div><div class="line">    def get_addr2_from(self, addr1_url):</div><div class="line">        addr1_url_msg = self.addr1_url_db.find_one(&#123;&apos;url&apos;: addr1_url&#125;)</div><div class="line">        tag1 = addr1_url_msg[&apos;tag1&apos;]</div><div class="line">        tag2 = addr1_url_msg[&apos;tag2&apos;]</div><div class="line">        addr1 = addr1_url_msg[&apos;addr1&apos;]</div><div class="line">        try:</div><div class="line">            r = self.s.get(addr1_url, proxies=self.proxies, timeout=self.timeout)</div><div class="line">            tree = etree.HTML(r.text)</div><div class="line">            addr2_items = tree.xpath(&apos;//div[@id=&quot;region-nav-sub&quot;]/a&apos;)</div><div class="line">            if len(addr2_items) == 0:</div><div class="line">                addr2 = &apos;no_sub&apos;</div><div class="line">                addr2_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;: addr1, &apos;addr2&apos;: addr2, &apos;url&apos;: addr1_url, &apos;status&apos;: &apos;addr1_not_sub&apos;&#125;</div><div class="line">                self.addr2_url_db.insert_one(addr2_url_msg)</div><div class="line">            else:</div><div class="line">                for i in addr2_items:</div><div class="line">                    addr2 = i.getchildren()[0].text</div><div class="line">                    url = i.attrib[&apos;href&apos;]</div><div class="line">                    addr2_url_msg = &#123;&apos;tag1&apos;:tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;:addr1, &apos;addr2&apos;: addr2, &apos;url&apos;:url&#125;</div><div class="line">                    self.addr2_url_db.insert_one(addr2_url_msg)</div><div class="line">            self.crawly_addr1_ok.insert_one(&#123;&apos;url&apos;: addr1_url&#125;)</div><div class="line">            self.linktime = 3  # 重置linktime</div><div class="line">            time.sleep(1)</div><div class="line">        except(</div><div class="line">        requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout,</div><div class="line">        requests.exceptions.ConnectionError):</div><div class="line">            if self.linktime &gt; 0:</div><div class="line">                print(&apos;爬取失败，现在重新链接&apos;)</div><div class="line">                self.get_addr2_from(addr1_url)</div><div class="line">                self.linktime -= 1</div><div class="line">            else:</div><div class="line">                print(&apos;&#123;&#125;爬取失败&apos;.format(addr1_url))</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    url = &apos;http://www.dianping.com/search/category/219/10/g0r0&apos;</div></pre></td></tr></table></figure></p><p><code>get_tag1_from</code>, <code>get_tag2_from</code>, <code>get_addr1_from</code>和<code>get_addr2_from</code>都采用了同样的逻辑：</p><ol><li>先请求url，获得response。</li><li>使用<code>etree.HTML</code>解析网页。</li><li>定位所要爬取元素的位置。</li><li>存储。</li></ol><p><code>shop_parsing.py</code>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line">#coding=utf-8</div><div class="line"></div><div class="line">import requests</div><div class="line">import random</div><div class="line">import pymongo</div><div class="line">import time</div><div class="line">from lxml import etree</div><div class="line">from config import USER_AGENT, PROXY, TIMEOUT, LINKTIME, PAGE_NUM_MAX</div><div class="line"></div><div class="line"></div><div class="line">client = pymongo.MongoClient(&apos;localhost&apos;, 27017)</div><div class="line">dp = client[&apos;dp&apos;]</div><div class="line">addr2_url_db = dp[&apos;addr2_url_db&apos;]  # 存储从addr1_url中成功爬取到的addr2_url</div><div class="line">dpshop = dp[&apos;dpshop&apos;]  # 存储从addr2中成功爬取到的dpshop_msg</div><div class="line">crawly_addr2_ok = dp[&apos;crawly_addr2_ok&apos;]  # 存储爬取成功的addr2_url</div><div class="line"></div><div class="line"></div><div class="line"># def callback(future):</div><div class="line">#     response, addr2_url = future.result()</div><div class="line">#     get_msg_from(response, addr2_url)</div><div class="line"></div><div class="line"></div><div class="line">def get_msg_from(response, addr2_url):</div><div class="line">    addr2_url_msg = addr2_url_db.find_one(&#123;&apos;url&apos;: addr2_url&#125;)</div><div class="line">    tag1 = addr2_url_msg[&apos;tag1&apos;]</div><div class="line">    tag2 = addr2_url_msg[&apos;tag2&apos;]</div><div class="line">    addr1 = addr2_url_msg[&apos;addr1&apos;]</div><div class="line">    addr2 = addr2_url_msg[&apos;addr2&apos;]</div><div class="line">    tree = etree.HTML(response.text)</div><div class="line">    # 提取标题信息</div><div class="line">    title_items = tree.xpath(&apos;//div[@id=&quot;shop-all-list&quot;]//div[@class=&quot;tit&quot;]/a[1]&apos;)</div><div class="line"></div><div class="line">    # 提取商户星级</div><div class="line">    star_items = tree.xpath(&apos;//div[@class=&quot;comment&quot;]/span&apos;)</div><div class="line"></div><div class="line">    # 提取评论数-人均价格</div><div class="line">    review_price_items = tree.xpath(&apos;//div[@class=&quot;comment&quot;]/a&apos;)</div><div class="line">    review_price_list = []</div><div class="line">    for i in review_price_items:</div><div class="line">        # 判断评论数-价格的a标签是否存在子元素，如果存在，则在子元素中提取评论数-价格，否则评论数-价格都为“None”</div><div class="line">        if i.getchildren():</div><div class="line">            text = i.getchildren()[0].text</div><div class="line">            review_price_list.append(text)</div><div class="line">        else:</div><div class="line">            review_price_list.append(&quot;None&quot;)</div><div class="line">    review_price_list = [review_price_list[i:i + 2] for i in range(0, len(review_price_list), 2)]</div><div class="line"></div><div class="line">    # 提取各项评分</div><div class="line">    score_items = tree.xpath(&apos;//span[@class=&quot;comment-list&quot;]/span/b&apos;)</div><div class="line">    score_items = [i.text for i in score_items]</div><div class="line">    score_items = [score_items[i:i + 3] for i in range(0, len(score_items), 3)]</div><div class="line"></div><div class="line">    # 提取菜系-商区-详细地址</div><div class="line">    tag_items = tree.xpath(&apos;//div[@class=&quot;tag-addr&quot;]//span&apos;)</div><div class="line">    tag_items = [i.text for i in tag_items]</div><div class="line">    tag_items = [tag_items[i:i + 3] for i in range(0, len(tag_items), 3)]</div><div class="line"></div><div class="line">    for title_url, star, review_price, score, tag_area_addr in zip(title_items, star_items, review_price_list,</div><div class="line">                                                                   score_items, tag_items):</div><div class="line">        dpshop_msg = &#123;</div><div class="line">            &apos;title&apos;: title_url.attrib[&apos;title&apos;],</div><div class="line">            &apos;url&apos;: title_url.attrib[&apos;href&apos;],</div><div class="line">            &apos;star&apos;: star.attrib[&apos;title&apos;],</div><div class="line">            &apos;review&apos;: review_price[0],</div><div class="line">            &apos;price&apos;: review_price[1],</div><div class="line">            &apos;taste&apos;: score[0],</div><div class="line">            &apos;env&apos;: score[1],</div><div class="line">            &apos;service&apos;: score[2],</div><div class="line">            &apos;tag1&apos;: tag1,</div><div class="line">            &apos;tag2&apos;: tag2,</div><div class="line">            &apos;addr1&apos;: addr1,</div><div class="line">            &apos;addr2&apos;: addr2,</div><div class="line">            &apos;full_addr&apos;: tag_area_addr[2]</div><div class="line">        &#125;</div><div class="line">        dpshop.insert_one(dpshop_msg)</div><div class="line">    crawly_addr2_ok.insert_one(&#123;&apos;url&apos;: addr2_url&#125;)</div><div class="line"></div><div class="line"></div><div class="line">def get_all_msg_from(addr2_url):</div><div class="line">    for page in range(1, PAGE_NUM_MAX+1):</div><div class="line">        result_url = &apos;&#123;&#125;p&#123;&#125;&apos;.format(addr2_url, page)</div><div class="line">        status_code, response = requests_url(result_url)</div><div class="line">        if status_code == &apos;link_bad&apos;:</div><div class="line">            print(&apos;请求失败，请在crawly_addr2_url_bad中查看请求失败url&apos;)</div><div class="line">            break</div><div class="line">        elif status_code == 404:</div><div class="line">            # addr2_bad = &#123;&apos;url&apos;: result_url, &apos;status&apos;: 404&#125;</div><div class="line">            print(&apos;&#123;&#125;没有相关商户&apos;.format(result_url))</div><div class="line">            break</div><div class="line">        else:</div><div class="line">            get_msg_from(response, addr2_url)</div><div class="line"></div><div class="line"></div><div class="line">def requests_url(result_url, linktime=LINKTIME):</div><div class="line">    s = requests.Session()</div><div class="line">    headers = random.choice(USER_AGENT)</div><div class="line">    s.headers.update(&#123;&apos;User-Agent&apos;: headers&#125;)</div><div class="line">    proxies = &#123;&apos;http&apos;:random.choice(PROXY)&#125;</div><div class="line">    try:</div><div class="line">        r = s.get(result_url, proxies=proxies, timeout=TIMEOUT)</div><div class="line">        time.sleep(1)</div><div class="line">        return (r.status_code, r)</div><div class="line">    except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout,</div><div class="line">           requests.exceptions.ConnectionError):</div><div class="line">        if linktime &gt; 0:</div><div class="line">            print(&apos;请求失败，现在重新链接&apos;)</div><div class="line">            linktime -= 1</div><div class="line">            requests_url(result_url, linktime)</div><div class="line">        else:</div><div class="line">            status_code = &apos;link_bad&apos;</div><div class="line">            response = None</div><div class="line">            return (status_code, response)</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    addr2_url = &apos;http://www.dianping.com/search/category/219/10/g217r27028p3&apos;</div></pre></td></tr></table></figure></p><ol><li><code>requests_url</code>函数用于请求网页，并返回状态码和response。</li><li><code>get_msg_from</code>函数用于解析源代码，定位元素，爬取并存储所有我们需要的商户信息。</li><li><code>get_all_msg_from</code>函数实现了多页码爬取，当遇到网页状态码为<code>404</code>时，代表此时没有相关商户，最后一页已经被爬取，自动跳出for循环。结束某个addr2_url的爬取。</li></ol><p><code>run.py</code>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"># coding=utf-8</div><div class="line"></div><div class="line">from parsing.cate_parsing import GetTagAddr</div><div class="line">from parsing.shop_parsing import get_all_msg_from, crawly_addr2_ok</div><div class="line">from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    start_url = &apos;http://www.dianping.com/search/category/219/10/g0r0&apos;</div><div class="line">    tag_addr_task = GetTagAddr()</div><div class="line"></div><div class="line">    # 爬取tag1_url</div><div class="line">    tag_addr_task.get_tag1_from(start_url)</div><div class="line">    print(&apos;tag1_url爬取完成&apos;)</div><div class="line"></div><div class="line">    # 爬取tag2_url</div><div class="line">    tag1_wait = set(i[&apos;url&apos;] for i in tag_addr_task.tag1_url_db.find())</div><div class="line">    tag1_ok = set(i[&apos;url&apos;] for i in tag_addr_task.crawly_tag1_ok.find())</div><div class="line">    tag1_task = tag1_wait - tag1_ok</div><div class="line">    for tag1_url in tag1_task:</div><div class="line">        tag_addr_task.get_tag2_from(tag1_url)</div><div class="line">    print(&apos;tag2_url爬取完成&apos;)</div><div class="line"></div><div class="line">    # 爬取addr1_url</div><div class="line">    tag2_wait = set(i[&apos;url&apos;] for i in tag_addr_task.tag2_url_db.find())</div><div class="line">    tag2_ok = set(i[&apos;url&apos;] for i in tag_addr_task.crawly_tag2_ok.find())</div><div class="line">    tag2_task = tag2_wait - tag2_ok</div><div class="line">    for tag2_url in tag2_task:</div><div class="line">        tag_addr_task.get_addr1_from(tag2_url)</div><div class="line">    print(&apos;addr1_url爬取完成&apos;)</div><div class="line"></div><div class="line">    # 爬取addr2_url</div><div class="line">    addr1_wait = set(i[&apos;url&apos;] for i in tag_addr_task.addr1_url_db.find())</div><div class="line">    addr1_ok = set(i[&apos;url&apos;] for i in tag_addr_task.crawly_addr1_ok.find())</div><div class="line">    addr1_task = addr1_wait - addr1_ok</div><div class="line">    with ProcessPoolExecutor(max_workers=4) as executor:</div><div class="line">        executor.map(tag_addr_task.get_addr2_from, addr1_task)</div><div class="line">    print(&apos;addr2_url爬取完成&apos;)</div><div class="line"></div><div class="line">    # 根据addr2_url爬取商户信息</div><div class="line">    addr2_wait = set(i[&apos;url&apos;] for i in tag_addr_task.addr2_url_db.find())</div><div class="line">    addr2_ok = set(i[&apos;url&apos;] for i in crawly_addr2_ok.find())</div><div class="line">    addr2_task = addr2_wait - addr2_ok</div><div class="line">    with ThreadPoolExecutor(max_workers=8) as executor:</div><div class="line">        for url in addr2_task:</div><div class="line">            v = executor.submit(get_all_msg_from, url)</div><div class="line">        executor.shutdown(wait=True)</div><div class="line">            # executor.map(get_all_msg_from, addr2_task, chunksize=50)</div><div class="line">    print(&apos;addr2_url_reslut_url爬取完成&apos;)</div></pre></td></tr></table></figure></p><ol><li>爬取<code>addr2_url</code>采用多进程爬取，多进程通过<code>concurrent.futures.ProcessPoolExecutor</code>实现。</li><li>爬取<code>dpshop_msg</code>采用了多线程爬取，多线程通过<code>congurrent.futures.ThreadPoolExecutor</code>实现。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;构建爬取大众点评美食数据的多进程爬虫（一）&quot;&gt;&lt;a href=&quot;#构建爬取大众点评美食数据的多进程爬虫（一）&quot; class=&quot;headerlink&quot; title=&quot;构建爬取大众点评美食数据的多进程爬虫（一）&quot;&gt;&lt;/a&gt;构建爬取大众点评美食数据的多进程爬虫（一）&lt;/
      
    
    </summary>
    
    
      <category term="Python爬虫" scheme="https://richardrw.github.io/tags/Python%E7%88%AC%E8%99%AB/"/>
    
      <category term="多进程" scheme="https://richardrw.github.io/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
      <category term="大众点评" scheme="https://richardrw.github.io/tags/%E5%A4%A7%E4%BC%97%E7%82%B9%E8%AF%84/"/>
    
  </entry>
  
  <entry>
    <title>从美食的角度看东莞（数据来源：大众点评）</title>
    <link href="https://richardrw.github.io/2017/10/20/%E4%BB%8E%E7%BE%8E%E9%A3%9F%E7%9A%84%E8%A7%92%E5%BA%A6%E7%9C%8B%E4%B8%9C%E8%8E%9E%EF%BC%88%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90%EF%BC%9A%E5%A4%A7%E4%BC%97%E7%82%B9%E8%AF%84%EF%BC%89/"/>
    <id>https://richardrw.github.io/2017/10/20/从美食的角度看东莞（数据来源：大众点评）/</id>
    <published>2017-10-20T13:35:45.000Z</published>
    <updated>2017-10-21T02:59:34.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从美食的角度看东莞（数据来源：大众点评）"><a href="#从美食的角度看东莞（数据来源：大众点评）" class="headerlink" title="从美食的角度看东莞（数据来源：大众点评）"></a>从美食的角度看东莞（数据来源：大众点评）</h1><p>先从为什么要做这个项目说起，事情的缘由是这样的，小编当时跟领导到别的城市外出出差一个星期，作为一个平时很少外出的人，别说是别的城市了，就连东莞这个地方我都还没走遍过，尤其是那些重要商区、美食店，平时跟朋友出去玩，都很尴尬不知道要吃什么好。现在跟领导外出别的城市，感觉自己是个刚出大山的农村小伙子，对外面的世界所知甚少。作为领导的助手，自然就要安排好这一星期的就餐地点。然而这就很难为我了，我连东莞有什么地方有好吃的都不知道，现在居然要我在别的城市安排一个星期的就餐作息表？这就很尴尬了。于是我每天都苦于到大众点评、美团等APP上搜索附近美食，但是还是不知道那些餐店的真实情况怎样，环境如何，味道如何。于是此时我就想，如果能一下子就知道某个城市或者某个区中最受欢迎的店有哪些，哪个镇或者哪个区最繁华，那该多好。<br>于是小编回来后就开始构思，接着就撸起袖子加油干，爬取了大众点评上1.5w多条数据，并做了个东莞美食商户数量热力图，从美食的角度看东莞。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx3.sinaimg.cn/large/006XG4i7gy1fkpnbkccanj30xc0m8myo.jpg" alt="东莞各镇街美食商户数量分布热力图" title="">                </div>                <div class="image-caption">东莞各镇街美食商户数量分布热力图</div>            </figure><br>（因为图片大小问题，莞城、洪梅和大岭山的标签没有显示出来，要解决这个问题，可以找张大点的图制作SVG）</p><p><a href="https://code.hcharts.cn/temp/MYNjyG/share" target="_blank" rel="external">东莞各镇街美食商户数量分布热力图-动态</a></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx2.sinaimg.cn/large/006XG4i7gy1fkp14l3pqrj311v0dnmxs.jpg" alt="东莞各镇街美食商户数量柱状图" title="">                </div>                <div class="image-caption">东莞各镇街美食商户数量柱状图</div>            </figure><p>可以看出，东城区商户数量最多，其次是南城区，从热力图来看，也可以看出东莞哪个镇街最繁华，也可以初步判断出哪些镇街经济情况比较好。因为一般经济越好的地区，其商业越繁华越发达。为了验证这个猜想，小编查了下东莞2016年各镇街经济实力排名情况<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx2.sinaimg.cn/large/006XG4i7gy1fkp14l6t1ij309k0l3ab8.jpg" alt="东莞各镇街2017年经济实力排名情况" title="">                </div>                <div class="image-caption">东莞各镇街2017年经济实力排名情况</div>            </figure><br>可以看出，经济实力与地区商户数量基本成正相关。这里说一个特例吧，那就是松山湖。本地人可能知道原因，那就是因为松山湖建立历史比较年轻，区域布局比较先进，目前仅有万科（好像是）这个大型商场，其他就以大学、高新技术产业区以及青山绿水为主，没有一脑子就开发许多商业区，所以就出现了这个经济实力与地区美食商户数量不匹配的情况。<br>接下来看看各星级商户占比情况如何<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx3.sinaimg.cn/large/006XG4i7gy1fkp14l3hpyj30sn0bb0t4.jpg" alt="东莞不同星级商户占比饼状图" title="">                </div>                <div class="image-caption">东莞不同星级商户占比饼状图</div>            </figure><br>准四星用户占73.98%，看来东莞的饮食服务业还是可以的（毕竟是广东啊哈哈）<br>再看看东莞各菜系商户数量情况如何<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://wx1.sinaimg.cn/large/006XG4i7gy1fkp14l2dofj311y0cvgm1.jpg" alt="东莞各菜系商户数量柱状图" title="">                </div>                <div class="image-caption">东莞各菜系商户数量柱状图</div>            </figure><br>小吃快餐最多，其次是面包甜点。小吃快餐最多的原因小编猜测可能有三：</p><ol><li>大多数公司中的员工中午都在公司叫外卖，外卖以小吃快餐为主，需求较大，由此催生出一批小吃快餐店，以满足市场需求。</li><li>开小吃快餐店的成本比较低，像沙县小吃什么的快餐店，随便会做个番茄炒蛋饭青瓜肉片饭就能开个快餐店了（哈哈），比面包甜点、西餐的开店成本低。</li><li>大多数小吃快餐店都选择加入大众点评平台，以提高销售量，增加利润。</li></ol><p>至于面包甜点有3155家，这让小编感到意外，没想到东莞居然有这么多面包店。<br>好了，暂时就分析到这里，小编最爱吃的就是面包了，赶紧去看看里面有哪些面包店的，一定要去尝尝。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;从美食的角度看东莞（数据来源：大众点评）&quot;&gt;&lt;a href=&quot;#从美食的角度看东莞（数据来源：大众点评）&quot; class=&quot;headerlink&quot; title=&quot;从美食的角度看东莞（数据来源：大众点评）&quot;&gt;&lt;/a&gt;从美食的角度看东莞（数据来源：大众点评）&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
      <category term="数据分析" scheme="https://richardrw.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="美食" scheme="https://richardrw.github.io/tags/%E7%BE%8E%E9%A3%9F/"/>
    
      <category term="东莞" scheme="https://richardrw.github.io/tags/%E4%B8%9C%E8%8E%9E/"/>
    
  </entry>
  
  <entry>
    <title>年轻人是否到北上广深发展的博弈论分析</title>
    <link href="https://richardrw.github.io/2017/09/25/%E5%B9%B4%E8%BD%BB%E4%BA%BA%E6%98%AF%E5%90%A6%E5%BA%94%E8%AF%A5%E5%88%B0%E5%8C%97%E4%B8%8A%E5%B9%BF%E6%B7%B1%E5%8F%91%E5%B1%95%E7%9A%84%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%88%86%E6%9E%90/"/>
    <id>https://richardrw.github.io/2017/09/25/年轻人是否应该到北上广深发展的博弈论分析/</id>
    <published>2017-09-25T14:05:11.000Z</published>
    <updated>2017-10-07T08:57:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、思考背景"><a href="#一、思考背景" class="headerlink" title="一、思考背景"></a>一、思考背景</h1><p>有人说，当你在一个岗位上工作时间超过1年的时候，你就会不自觉地思考，思考诸如你当前工作的意义是什么、这个岗位是否真的适合你等这些“终极问题”。博主现在已经毕业一年零三个月，实际工作时间已有一年零九个月，在这时间点上，大脑变的不自主地思考起一些关于人生的是非对错问题。引发博主思考这个问题的背景主要有三：</p><ol><li>曾为大学毕业生，也曾思考过到北上广深发展还是留守本地，驻地生根的问题。</li><li>身边有人说，到北上广深发展，只能当炮灰。在沉重的生活成本下，即使工资比二三线城市高，但是实际生活质量却不比二三线城市好，甚至更差。去了只能压榨自己，使自己成为一线城市的“蜡烛”，照亮了别人（成就了上层人士），燃烧了自己（折旧了自己）。</li><li>身边也有人说，到北上广深发展，机会更大。留在本地（二三线城市），固然稳定，生活压力也没那么大，但是一辈子都不会有什么变数，一生只能按现在的状态生活下去，极难遇到让自己人生变得更加美好的变数（机遇）。<br>到底谁说的对，谁说的错，是该往北上广深发展，还是驻地生根。不同的人在做这个决定的时候，他考量的是什么，决策依据是什么，决策背后的理论根据又是什么。此时慢慢地、慢慢地，博主想通过博弈论的角度，看能不能分析出什么，想知道，思考的结果又能不能为博主指明前往更美好生活的方向。</li></ol><hr><p>写着写着发现，这题目太过严肃认真了，提不起兴趣来写，姑且就先来吹吹水吧。<br>想必大家如果留意生活想象的话，应该不难发现这些现象：在足球比赛中，说说国足吧，一开始总是让人觉得放不开来打，等到落后比分的时候才奋力直追放开来打。在帆船比赛中，落后的选手总是不会老老实实地跟着领航者的路线，而是另谋航线以求超越。在LOL等电子竞技赛中，劣势的一方总是希望通过大龙来抢夺来争取一线赢机。为什么人们在处于劣势环境的情况下，总是追求那些很不稳定的方法呢？例如在篮球足球比赛中，落后一方总是采取多冒险的策略，去搞小动作去犯规。下面我们就来用数学先分析一下。我们假设在一场篮球比赛中，比赛的结果由比赛双方的能力和运气一起决定，你的取胜条件需满足：</p><pre><code>你的能力 + 你的运气 &gt; 对手的能力 + 对手的运气</code></pre><p>或者</p><pre><code>你的运气 - 对手的运气 &gt; 你的能力 - 对手的能力        L          &gt;         S</code></pre><p>在一场比赛中，你的能力、对手的能力基本是已经确定的，因为你不会像《龙珠》里的赛亚人那样，打着打着就变身为超级赛亚人提升战斗力，因此我们不妨将<strong> “你的能力 - 对手的能力” </strong>的差值S视为一确定的系数。那么剩下来，影响比赛结果的就是<strong> “你的运气 - 对手的运气“ 的差值L能否大于S </strong>。</p><p>假设对手的能力比你强，你处于劣势，此时S&gt;0，为正值。你要获胜，就必须使得“你的运气 - 对手的运气”的差值L大于S，即当且仅当L&gt;S，你才能获胜。那么我们要如何使得L&gt;S呢？运气这种东西，是我们能够控制的吗？</p><p>我们不妨从数学的角度来看“你的运气 - 对手的运气”的差值L。因为你的运气和对手的运气都是不确定的，必然，“你的运气 - 对手的运气”的差值L也是不确定的，即L的值存在概率性，因此不妨假设L的概率分布是正态曲线（如下图中的实线曲线）。横轴x的任意点，代表L可能的取值，其到曲线上的高y值代表L取某值时对应的概率，因此曲线在两点间覆盖的面积等于L取值位于这两点间的概率。<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/Snip20171007_1.png" alt="L取值概率分布图" title="">                </div>                <div class="image-caption">L取值概率分布图</div>            </figure><br>由图可见，你取胜的概率为图中网格的阴影面积，即对于所有L可能的取值，满足L&gt;S的取值的概率之和。<br>到这里，我们已经知道“你的运气 - 对手的运气”的数学上的意义了。那么回到我们之前的问题：运气这种东西，是我们能够控制的吗？</p><p>答案是可以的。或许从感性理解上，这会很不可思议，但是从数学的角度看，改变运气，就是让你改变“你的运气”的概率分布，使得“你的运气 - 对手的运气”大于S变得更加可能。那么我们需要怎么做呢？还是先从数学角度看。让“你的运气 - 对手的运气”的值大于S变得更加可能，也就是让你增大图中的网格阴影面积。</p><p>问：怎么增大？<br>答：让正态曲线更加平坦即可。（如变成上图中的阴线曲线）</p><p>曲线变得更平坦后，由图可见，L&gt;S的面积增加了斜线阴影部分，也就是说L&gt;S的概率更大了。在这种情况下，你获胜的概率也就提升了。</p><p>曲线变得更平坦，意味着L的取值更具不确定性。在实际生活中，你一般只需采取更冒险、更激进的策略就能达到让“你的运气”的概率分布曲线变得更加平台的效果。就如篮球比赛中，对方实力比你强，你处于落后状态，这时，如果你陈规蹈矩，按部就班，那么你是很难有反超的机会的，因为你只能依靠进攻得分，而那些博犯规罚球得分等“非正常”得分你就很少有机会碰到，因为你打得很稳。相反，如果你采取冒险、更加激进的策略，你活得博犯规罚球得分的机会就会变大，得分机会变大，也就意味着你反超的机会变大。</p><p>因此，如果你面对一个比你强大的对手，而你又处于劣势状态，那么你就需要采取一些冒险的策略来打破这种“对手比你厉害，你处于劣势“的稳定状态，从而提高你的胜率。</p><p>回到标题中的问题，答案可以从上述中得出总结，只不过在这个比赛中，对手可能不是别人，而可能是你自己，或者整个大环境。你此时需要衡量一下自己的处境，自己能力对于大环境来说，是强是弱，自己的处境是劣势还是优势，在根据综合判断，看是否需要采取一些冒险的策略来打破平衡、打破稳定，提高自己的胜率。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、思考背景&quot;&gt;&lt;a href=&quot;#一、思考背景&quot; class=&quot;headerlink&quot; title=&quot;一、思考背景&quot;&gt;&lt;/a&gt;一、思考背景&lt;/h1&gt;&lt;p&gt;有人说，当你在一个岗位上工作时间超过1年的时候，你就会不自觉地思考，思考诸如你当前工作的意义是什么、这个岗位
      
    
    </summary>
    
    
      <category term="博弈论" scheme="https://richardrw.github.io/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/"/>
    
      <category term="稳定性" scheme="https://richardrw.github.io/tags/%E7%A8%B3%E5%AE%9A%E6%80%A7/"/>
    
      <category term="风险" scheme="https://richardrw.github.io/tags/%E9%A3%8E%E9%99%A9/"/>
    
      <category term="均衡" scheme="https://richardrw.github.io/tags/%E5%9D%87%E8%A1%A1/"/>
    
      <category term="北上广深" scheme="https://richardrw.github.io/tags/%E5%8C%97%E4%B8%8A%E5%B9%BF%E6%B7%B1/"/>
    
  </entry>
  
  <entry>
    <title>sublime如何添加python3版本以及中文乱码问题</title>
    <link href="https://richardrw.github.io/2017/09/24/sublime%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0python3%E7%89%88%E6%9C%AC%E4%BB%A5%E5%8F%8A%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98/"/>
    <id>https://richardrw.github.io/2017/09/24/sublime如何添加python3版本以及中文乱码问题/</id>
    <published>2017-09-23T16:05:05.000Z</published>
    <updated>2017-09-24T01:22:06.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、当同时安装了python2和python3时，如何让sublime-text同时支持？"><a href="#一、当同时安装了python2和python3时，如何让sublime-text同时支持？" class="headerlink" title="一、当同时安装了python2和python3时，如何让sublime text同时支持？"></a>一、当同时安装了python2和python3时，如何让sublime text同时支持？</h1><ol><li><p>在sublime text中选择Toos—&gt;Build System—&gt;New Build System，此时会创建一个新文件，内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line"> &quot;shell_cmd&quot;: &quot;make&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>将原有内容删除，把以下内容复制进去：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;cmd&quot;: [&quot;/Library/Frameworks/Python.framework/Versions/3.6/bin/python3&quot;, &quot;-u&quot;, &quot;$file&quot;],</div><div class="line">    &quot;file_regex&quot;: &quot;^[ ]*File \&quot;(...*?)\&quot;, line ([0-9]*)&quot;,</div><div class="line">    &quot;selector&quot;: &quot;source.python&quot;,</div><div class="line">    &quot;env&quot;: &#123;&quot;LANG&quot;: &quot;en_US.UTF-8&quot;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>注意：</strong> <code>/Library/Frameworks/Python.framework/Versions/3.6/bin/python3</code>为你安装python3的路径</p></li><li>查看你python2或python3的路径方法。在终端／cmd下执行    <code>which python</code>获取python2路径，执行<code>which python3</code>获取python3路径，复制替换<code>&quot;cmd&quot;</code>中python的路径即可。</li><li><code>&quot;env&quot;: {&quot;LANG&quot;: &quot;en_US.UTF-8&quot;</code>的作用是为了正常地显示中文</li><li>将文件保存为<code>Python3.sublime-build</code>，路径为sublime安装目录下的<code>Packages</code>文件夹</li></ol><h1 id="二、使用ConvertToUTF8解决中文乱码"><a href="#二、使用ConvertToUTF8解决中文乱码" class="headerlink" title="二、使用ConvertToUTF8解决中文乱码"></a>二、使用ConvertToUTF8解决中文乱码</h1><ol><li>使用Ctrl+Shift+P打开Package Control，输入install package按回车，再搜索ConvertToUTF8来安装插件</li><li>安装完后再次使用Ctrl+Shift+P打开Package Control，这次输入ConvertToUTF8，回车，再选择UTF-8编码即可。这样就会以utf-8的编码格式编辑文件。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、当同时安装了python2和python3时，如何让sublime-text同时支持？&quot;&gt;&lt;a href=&quot;#一、当同时安装了python2和python3时，如何让sublime-text同时支持？&quot; class=&quot;headerlink&quot; title=&quot;一、
      
    
    </summary>
    
    
      <category term="sublime" scheme="https://richardrw.github.io/tags/sublime/"/>
    
      <category term="python" scheme="https://richardrw.github.io/tags/python/"/>
    
      <category term="utf-8" scheme="https://richardrw.github.io/tags/utf-8/"/>
    
  </entry>
  
  <entry>
    <title>test_my_site</title>
    <link href="https://richardrw.github.io/2017/09/16/test-my-site/"/>
    <id>https://richardrw.github.io/2017/09/16/test-my-site/</id>
    <published>2017-09-16T07:23:04.000Z</published>
    <updated>2017-09-16T07:23:04.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://richardrw.github.io/2017/09/16/hello-world/"/>
    <id>https://richardrw.github.io/2017/09/16/hello-world/</id>
    <published>2017-09-16T07:08:10.000Z</published>
    <updated>2017-09-16T07:08:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
