<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一） | Richard‘s Blog | 记录敲码过程中遇到的细节和问题</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="Python爬虫,多进程,大众点评">
    <meta name="description" content="构建爬取大众点评美食数据的多进程爬虫（一）一、要抓取哪些数据以东莞美食为例大众点评-美食-东莞，可以看到，在美食页面，我们想要爬取的信息有：  商户标题 星级 评论数 人均价格 各项评分 菜系 地址  二、分析页面（一）多观察页面通过点击小吃快餐，我们看到小吃快餐下一共有50个页面。再点击面包甜点，发现也只有50个页面。接下来我们看看面包甜点-东城区，发现最多也只有50个页面。如此类推，我们大概可">
<meta name="keywords" content="Python爬虫,多进程,大众点评">
<meta property="og:type" content="article">
<meta property="og:title" content="构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）">
<meta property="og:url" content="https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/index.html">
<meta property="og:site_name" content="Richard‘s Blog">
<meta property="og:description" content="构建爬取大众点评美食数据的多进程爬虫（一）一、要抓取哪些数据以东莞美食为例大众点评-美食-东莞，可以看到，在美食页面，我们想要爬取的信息有：  商户标题 星级 评论数 人均价格 各项评分 菜系 地址  二、分析页面（一）多观察页面通过点击小吃快餐，我们看到小吃快餐下一共有50个页面。再点击面包甜点，发现也只有50个页面。接下来我们看看面包甜点-东城区，发现最多也只有50个页面。如此类推，我们大概可">
<meta property="og:image" content="http://wx1.sinaimg.cn/large/006XG4i7gy1fkpn3b1n9rj30fc0gpdgi.jpg">
<meta property="og:updated_time" content="2017-11-04T14:29:19.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）">
<meta name="twitter:description" content="构建爬取大众点评美食数据的多进程爬虫（一）一、要抓取哪些数据以东莞美食为例大众点评-美食-东莞，可以看到，在美食页面，我们想要爬取的信息有：  商户标题 星级 评论数 人均价格 各项评分 菜系 地址  二、分析页面（一）多观察页面通过点击小吃快餐，我们看到小吃快餐下一共有50个页面。再点击面包甜点，发现也只有50个页面。接下来我们看看面包甜点-东城区，发现最多也只有50个页面。如此类推，我们大概可">
<meta name="twitter:image" content="http://wx1.sinaimg.cn/large/006XG4i7gy1fkpn3b1n9rj30fc0gpdgi.jpg">
    
        <link rel="alternate" type="application/atom+xml" title="Richard‘s Blog" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/author.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Richard</h5>
          <a href="mailto:weichang321@gmail.com" title="weichang321@gmail.com" class="mail">weichang321@gmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/richardrw" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="http://www.weibo.com/6378627091" target="_blank" >
                <i class="icon icon-lg icon-weibo"></i>
                Weibo
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-10-20T14:07:02.000Z" itemprop="datePublished" class="page-time">
  2017-10-20
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#构建爬取大众点评美食数据的多进程爬虫（一）"><span class="post-toc-number">1.</span> <span class="post-toc-text">构建爬取大众点评美食数据的多进程爬虫（一）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#一、要抓取哪些数据"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">一、要抓取哪些数据</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#二、分析页面"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">二、分析页面</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（一）多观察页面"><span class="post-toc-number">1.2.1.</span> <span class="post-toc-text">（一）多观察页面</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（二）分析页面结构"><span class="post-toc-number">1.2.2.</span> <span class="post-toc-text">（二）分析页面结构</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#三、设计爬虫"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">三、设计爬虫</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（一）爬虫思路"><span class="post-toc-number">1.3.1.</span> <span class="post-toc-text">（一）爬虫思路</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（二）构建代码"><span class="post-toc-number">1.3.2.</span> <span class="post-toc-text">（二）构建代码</span></a></li></ol></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-构建爬取大众点评美食数据的多进程爬虫（一）"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-10-20 22:07:02" datetime="2017-10-20T14:07:02.000Z"  itemprop="datePublished">2017-10-20</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="构建爬取大众点评美食数据的多进程爬虫（一）"><a href="#构建爬取大众点评美食数据的多进程爬虫（一）" class="headerlink" title="构建爬取大众点评美食数据的多进程爬虫（一）"></a>构建爬取大众点评美食数据的多进程爬虫（一）</h1><h2 id="一、要抓取哪些数据"><a href="#一、要抓取哪些数据" class="headerlink" title="一、要抓取哪些数据"></a>一、要抓取哪些数据</h2><p>以东莞美食为例<a href="http://www.dianping.com/search/category/219/10/g0r0" target="_blank" rel="external">大众点评-美食-东莞</a>，可以看到，在美食页面，我们想要爬取的信息有：</p>
<ol>
<li>商户标题</li>
<li>星级</li>
<li>评论数</li>
<li>人均价格</li>
<li>各项评分</li>
<li>菜系</li>
<li>地址</li>
</ol>
<h2 id="二、分析页面"><a href="#二、分析页面" class="headerlink" title="二、分析页面"></a>二、分析页面</h2><h3 id="（一）多观察页面"><a href="#（一）多观察页面" class="headerlink" title="（一）多观察页面"></a>（一）多观察页面</h3><p>通过点击<a href="http://www.dianping.com/search/category/219/10/g112" target="_blank" rel="external">小吃快餐</a>，我们看到小吃快餐下一共有50个页面。再点击<a href="http://www.dianping.com/search/category/219/10/g117" target="_blank" rel="external">面包甜点</a>，发现也只有50个页面。接下来我们看看<a href="http://www.dianping.com/search/category/219/10/g117r434" target="_blank" rel="external">面包甜点-东城区</a>，发现最多也只有50个页面。如此类推，我们大概可以确定，某菜系-某区域最多只有50个页面，由此基本可以判断，这应该是大众点评做大反爬虫措施。对此，我们在稍后的爬虫设计中，不能简单地从某一菜系下爬取所有页面或者从某一区域下爬取所有页面，因为单独选定菜系或者区域，服务器最多只返回50个页面给你。<strong> 因此我们需要将菜系-区域组合起来爬取，尽可能多的爬取商户信息。 </strong><br>通过观察，我们发现在一个网页中，如：<a href="http://www.dianping.com/search/category/219/10/g117r434" target="_blank" rel="external">http://www.dianping.com/search/category/219/10/g117r434</a><br><code>g117</code>代表菜系，<code>r434</code>代表区域，因此我们可以获取所有的菜系链接，接着在菜系链接的基础上获取菜系-区域链接，这就相当于在用浏览器浏览时，先选定了某一菜系，再选定某一区域。</p>
<h3 id="（二）分析页面结构"><a href="#（二）分析页面结构" class="headerlink" title="（二）分析页面结构"></a>（二）分析页面结构</h3><p>这里就是要通过定位来选定我们需要的元素。如一级菜系，它所处的位置是<code>id=&quot;classfy</code>的<code>div</code>标签下的<code>a</code>标签中。<br>商户标题，它所处的位置是<code>class=&quot;tit&quot;</code>的<code>div</code>标签下的第一个<code>a</code>标签中。通过一个个地查看，我们可以得出我们要爬取的7个商户信息的位置，方便我们后续设计爬虫时进行定位。</p>
<h2 id="三、设计爬虫"><a href="#三、设计爬虫" class="headerlink" title="三、设计爬虫"></a>三、设计爬虫</h2><h3 id="（一）爬虫思路"><a href="#（一）爬虫思路" class="headerlink" title="（一）爬虫思路"></a>（一）爬虫思路</h3><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://wx1.sinaimg.cn/large/006XG4i7gy1fkpn3b1n9rj30fc0gpdgi.jpg" alt="爬虫流程图" title="">
                </div>
                <div class="image-caption">爬虫流程图</div>
            </figure>
<ol>
<li>从<code>start_url</code>开始，爬取所有一级菜系链接，得到<code>tag1_url</code>，存入数据库。</li>
<li>从数据库中读取<code>tag1_url</code>，爬取所有二级菜系链接，得到<code>tag2_url</code>，存入数据库。</li>
<li>从数据库中读取<code>tag2_url</code>，爬取所有一级区域链接，得到<code>addr1_url</code>，存入数据库。</li>
<li>从数据库中读取<code>addr1_url</code>，爬取所有二级区域链接，得到<code>addr2_url</code>，存入数据库。</li>
<li>从数据库中读取<code>addr2_url</code>，爬取所有商户信息，得到<code>dpshop_msg</code>，存入数据库。<br>1-4目的都是一样，为了获取最终要爬取的页面链接，5就是为了实际爬取上述7个商户信息，所以我们把1-4写到<code>cate_parsing.py</code>文件中，把5写到<code>shop_parsing.py</code>文件中。另外，为了应对反爬虫，我们将用于伪装的<code>User-Agent</code>和<code>代理IP</code>等一些爬虫参数写到<code>config.py</code>文件中。<h3 id="（二）构建代码"><a href="#（二）构建代码" class="headerlink" title="（二）构建代码"></a>（二）构建代码</h3><code>config.py</code>代码如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">#coding=utf-8</div><div class="line"></div><div class="line"></div><div class="line">#伪装浏览器</div><div class="line">USER_AGENT = [</div><div class="line">    &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)&apos;,</div><div class="line">    &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36&apos;,</div><div class="line">    &apos;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11&apos;,</div><div class="line">    &apos;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16&apos;,</div><div class="line">    &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0&apos;,</div><div class="line">    &apos;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&apos;</div><div class="line">    ]</div><div class="line"></div><div class="line"># 代理IP</div><div class="line">PROXY = [</div><div class="line">    &apos;111.13.111.184:80&apos;,</div><div class="line">    &apos;49.119.164.175:80&apos;,</div><div class="line">    &apos;61.136.163.245:3128&apos;,</div><div class="line">    &apos;116.199.2.210:80&apos;,</div><div class="line">    &apos;116.199.2.209:80&apos;,</div><div class="line">    &apos;116.199.115.79:80&apos;,</div><div class="line">    &apos;116.199.2.196:80&apos;,</div><div class="line">    &apos;121.40.199.105:80&apos;,</div><div class="line">    &apos;125.77.25.118:80&apos;,</div><div class="line">    &apos;122.228.253.55:808&apos;</div><div class="line">    ]</div><div class="line"></div><div class="line">TIMEOUT = 5</div><div class="line"></div><div class="line">LINKTIME = 3</div><div class="line"></div><div class="line">PAGE_NUM_MAX = 50</div></pre></td></tr></table></figure>
</li>
</ol>
<p><code>cate_parsing.py</code>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div></pre></td><td class="code"><pre><div class="line">#coding=utf-8</div><div class="line"></div><div class="line">import requests</div><div class="line">import random</div><div class="line">import pymongo</div><div class="line">import time</div><div class="line">from lxml import etree</div><div class="line">from config import USER_AGENT, PROXY, TIMEOUT, LINKTIME</div><div class="line"></div><div class="line">client = pymongo.MongoClient(&apos;localhost&apos;, 27017)</div><div class="line">dp = client[&apos;dp&apos;]</div><div class="line"></div><div class="line"></div><div class="line"># 爬取一级菜系、二级菜系、二级菜系下的一级地址</div><div class="line">class GetTagAddr(object):</div><div class="line">    headers = random.choice(USER_AGENT)</div><div class="line">    proxies = &#123;&apos;http&apos;:random.choice(PROXY)&#125;</div><div class="line">    s = requests.Session()</div><div class="line">    s.headers.update(&#123;&apos;User-Agent&apos;:headers&#125;)</div><div class="line">    linktime = LINKTIME</div><div class="line">    timeout = TIMEOUT</div><div class="line">    tag1_url_db = dp[&apos;tag1_url_db&apos;]             # 存储从start_url中成功爬取到的tag1_url</div><div class="line">    tag2_url_db = dp[&apos;tag2_url_db&apos;]                    # 存储从tag1_url中成功爬取到的tag2_url</div><div class="line">    crawly_tag1_ok = dp[&apos;crawly_tag1_ok&apos;]    # 存储爬取成功的tag1_url</div><div class="line">    addr1_url_db = dp[&apos;addr1_url_db&apos;]            # 存储从tag2_url中成功爬取到的addr1_url</div><div class="line">    crawly_tag2_ok = dp[&apos;crawly_tag2_ok&apos;]    # 存储爬取成功的tag2_url</div><div class="line">    addr2_url_db = dp[&apos;addr2_url_db&apos;]            # 存储从addr1_url中成功爬取到的addr2_url</div><div class="line">    crawly_addr1_ok = dp[&apos;crawly_addr1_ok&apos;]  # 存储爬取成功的addr1_url</div><div class="line"></div><div class="line"></div><div class="line">    def get_tag1_from(self, start_url):</div><div class="line">        try:</div><div class="line">            r = self.s.get(start_url, proxies=self.proxies, timeout=self.timeout)</div><div class="line">            tree = etree.HTML(r.text)</div><div class="line">            tag1_items = tree.xpath(&apos;//div[@id=&quot;classfy&quot;]/a&apos;)</div><div class="line">            for i in tag1_items:</div><div class="line">                tag1 = i.getchildren()[0].text</div><div class="line">                url = i.attrib[&apos;href&apos;]</div><div class="line">                tag1_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;url&apos;: url, &apos;status&apos;: &apos;ok&apos;&#125;</div><div class="line">                self.tag1_url_db.insert_one(tag1_url_msg)</div><div class="line">            self.linktime = 3    # 重置linktime</div><div class="line">            time.sleep(1)</div><div class="line">        except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):</div><div class="line">            if self.linktime &gt; 0:</div><div class="line">                print(&apos;爬取失败，现在重新链接&apos;)</div><div class="line">                self.get_tag1_from(start_url)</div><div class="line">                self.linktime -= 1</div><div class="line">            else:</div><div class="line">                print(&apos;&#123;&#125;爬取失败&apos;.format(start_url))</div><div class="line"></div><div class="line"></div><div class="line">    def get_tag2_from(self, tag1_url):</div><div class="line">        tag1_url_msg = self.tag1_url_db.find_one(&#123;&apos;url&apos;: tag1_url&#125;)</div><div class="line">        tag1 = tag1_url_msg[&apos;tag1&apos;]</div><div class="line">        try:</div><div class="line">            r = self.s.get(tag1_url, proxies=self.proxies, timeout=self.timeout)</div><div class="line">            tree = etree.HTML(r.text)</div><div class="line">            tag2_items = tree.xpath(&apos;//div[@id=&quot;classfy-sub&quot;]/a&apos;)</div><div class="line">            if len(tag2_items) == 0:</div><div class="line">                tag2 = &apos;no_sub&apos;</div><div class="line">                tag2_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;url&apos;: tag1_url, &apos;status&apos;: &apos;tag1_not_sub&apos;&#125;</div><div class="line">                self.tag2_url_db.insert_one(tag2_url_msg)</div><div class="line">            else:</div><div class="line">                for i in tag2_items:</div><div class="line">                    tag2 = i.getchildren()[0].text</div><div class="line">                    url = i.attrib[&apos;href&apos;]</div><div class="line">                    tag2_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;url&apos;: url, &apos;status&apos;: &apos;ok&apos;&#125;</div><div class="line">                    self.tag2_url_db.insert_one(tag2_url_msg)</div><div class="line">            self.crawly_tag1_ok.insert_one(&#123;&apos;url&apos;: tag1_url&#125;)</div><div class="line">            self.linktime = 3    # 重置linktime</div><div class="line">            time.sleep(1)</div><div class="line">        except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):</div><div class="line">            if self.linktime &gt; 0:</div><div class="line">                print(&apos;爬取失败，现在重新链接&apos;)</div><div class="line">                self.get_tag2_from(tag1_url)</div><div class="line">                self.linktime -= 1</div><div class="line">            else:</div><div class="line">                print(&apos;&#123;&#125;爬取失败&apos;.format(tag1_url))</div><div class="line"></div><div class="line"></div><div class="line">    def get_addr1_from(self, tag2_url):</div><div class="line">        tag2_url_msg = self.tag2_url_db.find_one(&#123;&apos;url&apos;: tag2_url&#125;)</div><div class="line">        tag1 = tag2_url_msg[&apos;tag1&apos;]</div><div class="line">        tag2 = tag2_url_msg[&apos;tag2&apos;]</div><div class="line">        try:</div><div class="line">            r = self.s.get(tag2_url, proxies=self.proxies, timeout=self.timeout)</div><div class="line">            tree = etree.HTML(r.text)</div><div class="line">            addr1_items = tree.xpath(&apos;//div[@id=&quot;region-nav&quot;]/a&apos;)</div><div class="line">            for i in addr1_items:</div><div class="line">                addr1 = i.getchildren()[0].text</div><div class="line">                url = i.attrib[&apos;href&apos;]</div><div class="line">                addr1_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;: addr1, &apos;url&apos;: url&#125;</div><div class="line">                self.addr1_url_db.insert_one(addr1_url_msg)</div><div class="line">            self.crawly_tag2_ok.insert_one(&#123;&apos;url&apos;: tag2_url&#125;)</div><div class="line">            self.linktime = 3  # 重置linktime</div><div class="line">            time.sleep(1)</div><div class="line">        except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout,</div><div class="line">               requests.exceptions.ConnectionError):</div><div class="line">            if self.linktime &gt; 0:</div><div class="line">                print(&apos;爬取失败，现在重新链接&apos;)</div><div class="line">                self.get_addr1_from(tag2_url)</div><div class="line">                self.linktime -= 1</div><div class="line">            else:</div><div class="line">                print(&apos;&#123;&#125;爬取失败&apos;.format(tag2_url))</div><div class="line"></div><div class="line"></div><div class="line">    def get_addr2_from(self, addr1_url):</div><div class="line">        addr1_url_msg = self.addr1_url_db.find_one(&#123;&apos;url&apos;: addr1_url&#125;)</div><div class="line">        tag1 = addr1_url_msg[&apos;tag1&apos;]</div><div class="line">        tag2 = addr1_url_msg[&apos;tag2&apos;]</div><div class="line">        addr1 = addr1_url_msg[&apos;addr1&apos;]</div><div class="line">        try:</div><div class="line">            r = self.s.get(addr1_url, proxies=self.proxies, timeout=self.timeout)</div><div class="line">            tree = etree.HTML(r.text)</div><div class="line">            addr2_items = tree.xpath(&apos;//div[@id=&quot;region-nav-sub&quot;]/a&apos;)</div><div class="line">            if len(addr2_items) == 0:</div><div class="line">                addr2 = &apos;no_sub&apos;</div><div class="line">                addr2_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;: addr1, &apos;addr2&apos;: addr2, &apos;url&apos;: addr1_url, &apos;status&apos;: &apos;addr1_not_sub&apos;&#125;</div><div class="line">                self.addr2_url_db.insert_one(addr2_url_msg)</div><div class="line">            else:</div><div class="line">                for i in addr2_items:</div><div class="line">                    addr2 = i.getchildren()[0].text</div><div class="line">                    url = i.attrib[&apos;href&apos;]</div><div class="line">                    addr2_url_msg = &#123;&apos;tag1&apos;:tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;:addr1, &apos;addr2&apos;: addr2, &apos;url&apos;:url&#125;</div><div class="line">                    self.addr2_url_db.insert_one(addr2_url_msg)</div><div class="line">            self.crawly_addr1_ok.insert_one(&#123;&apos;url&apos;: addr1_url&#125;)</div><div class="line">            self.linktime = 3  # 重置linktime</div><div class="line">            time.sleep(1)</div><div class="line">        except(</div><div class="line">        requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout,</div><div class="line">        requests.exceptions.ConnectionError):</div><div class="line">            if self.linktime &gt; 0:</div><div class="line">                print(&apos;爬取失败，现在重新链接&apos;)</div><div class="line">                self.get_addr2_from(addr1_url)</div><div class="line">                self.linktime -= 1</div><div class="line">            else:</div><div class="line">                print(&apos;&#123;&#125;爬取失败&apos;.format(addr1_url))</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    url = &apos;http://www.dianping.com/search/category/219/10/g0r0&apos;</div></pre></td></tr></table></figure></p>
<p><code>get_tag1_from</code>, <code>get_tag2_from</code>, <code>get_addr1_from</code>和<code>get_addr2_from</code>都采用了同样的逻辑：</p>
<ol>
<li>先请求url，获得response。</li>
<li>使用<code>etree.HTML</code>解析网页。</li>
<li>定位所要爬取元素的位置。</li>
<li>存储。</li>
</ol>
<p><code>shop_parsing.py</code>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line">#coding=utf-8</div><div class="line"></div><div class="line">import requests</div><div class="line">import random</div><div class="line">import pymongo</div><div class="line">import time</div><div class="line">from lxml import etree</div><div class="line">from config import USER_AGENT, PROXY, TIMEOUT, LINKTIME, PAGE_NUM_MAX</div><div class="line"></div><div class="line"></div><div class="line">client = pymongo.MongoClient(&apos;localhost&apos;, 27017)</div><div class="line">dp = client[&apos;dp&apos;]</div><div class="line">addr2_url_db = dp[&apos;addr2_url_db&apos;]  # 存储从addr1_url中成功爬取到的addr2_url</div><div class="line">dpshop = dp[&apos;dpshop&apos;]  # 存储从addr2中成功爬取到的dpshop_msg</div><div class="line">crawly_addr2_ok = dp[&apos;crawly_addr2_ok&apos;]  # 存储爬取成功的addr2_url</div><div class="line"></div><div class="line"></div><div class="line"># def callback(future):</div><div class="line">#     response, addr2_url = future.result()</div><div class="line">#     get_msg_from(response, addr2_url)</div><div class="line"></div><div class="line"></div><div class="line">def get_msg_from(response, addr2_url):</div><div class="line">    addr2_url_msg = addr2_url_db.find_one(&#123;&apos;url&apos;: addr2_url&#125;)</div><div class="line">    tag1 = addr2_url_msg[&apos;tag1&apos;]</div><div class="line">    tag2 = addr2_url_msg[&apos;tag2&apos;]</div><div class="line">    addr1 = addr2_url_msg[&apos;addr1&apos;]</div><div class="line">    addr2 = addr2_url_msg[&apos;addr2&apos;]</div><div class="line">    tree = etree.HTML(response.text)</div><div class="line">    # 提取标题信息</div><div class="line">    title_items = tree.xpath(&apos;//div[@id=&quot;shop-all-list&quot;]//div[@class=&quot;tit&quot;]/a[1]&apos;)</div><div class="line"></div><div class="line">    # 提取商户星级</div><div class="line">    star_items = tree.xpath(&apos;//div[@class=&quot;comment&quot;]/span&apos;)</div><div class="line"></div><div class="line">    # 提取评论数-人均价格</div><div class="line">    review_price_items = tree.xpath(&apos;//div[@class=&quot;comment&quot;]/a&apos;)</div><div class="line">    review_price_list = []</div><div class="line">    for i in review_price_items:</div><div class="line">        # 判断评论数-价格的a标签是否存在子元素，如果存在，则在子元素中提取评论数-价格，否则评论数-价格都为“None”</div><div class="line">        if i.getchildren():</div><div class="line">            text = i.getchildren()[0].text</div><div class="line">            review_price_list.append(text)</div><div class="line">        else:</div><div class="line">            review_price_list.append(&quot;None&quot;)</div><div class="line">    review_price_list = [review_price_list[i:i + 2] for i in range(0, len(review_price_list), 2)]</div><div class="line"></div><div class="line">    # 提取各项评分</div><div class="line">    score_items = tree.xpath(&apos;//span[@class=&quot;comment-list&quot;]/span/b&apos;)</div><div class="line">    score_items = [i.text for i in score_items]</div><div class="line">    score_items = [score_items[i:i + 3] for i in range(0, len(score_items), 3)]</div><div class="line"></div><div class="line">    # 提取菜系-商区-详细地址</div><div class="line">    tag_items = tree.xpath(&apos;//div[@class=&quot;tag-addr&quot;]//span&apos;)</div><div class="line">    tag_items = [i.text for i in tag_items]</div><div class="line">    tag_items = [tag_items[i:i + 3] for i in range(0, len(tag_items), 3)]</div><div class="line"></div><div class="line">    for title_url, star, review_price, score, tag_area_addr in zip(title_items, star_items, review_price_list,</div><div class="line">                                                                   score_items, tag_items):</div><div class="line">        dpshop_msg = &#123;</div><div class="line">            &apos;title&apos;: title_url.attrib[&apos;title&apos;],</div><div class="line">            &apos;url&apos;: title_url.attrib[&apos;href&apos;],</div><div class="line">            &apos;star&apos;: star.attrib[&apos;title&apos;],</div><div class="line">            &apos;review&apos;: review_price[0],</div><div class="line">            &apos;price&apos;: review_price[1],</div><div class="line">            &apos;taste&apos;: score[0],</div><div class="line">            &apos;env&apos;: score[1],</div><div class="line">            &apos;service&apos;: score[2],</div><div class="line">            &apos;tag1&apos;: tag1,</div><div class="line">            &apos;tag2&apos;: tag2,</div><div class="line">            &apos;addr1&apos;: addr1,</div><div class="line">            &apos;addr2&apos;: addr2,</div><div class="line">            &apos;full_addr&apos;: tag_area_addr[2]</div><div class="line">        &#125;</div><div class="line">        dpshop.insert_one(dpshop_msg)</div><div class="line">    crawly_addr2_ok.insert_one(&#123;&apos;url&apos;: addr2_url&#125;)</div><div class="line"></div><div class="line"></div><div class="line">def get_all_msg_from(addr2_url):</div><div class="line">    for page in range(1, PAGE_NUM_MAX+1):</div><div class="line">        result_url = &apos;&#123;&#125;p&#123;&#125;&apos;.format(addr2_url, page)</div><div class="line">        status_code, response = requests_url(result_url)</div><div class="line">        if status_code == &apos;link_bad&apos;:</div><div class="line">            print(&apos;请求失败，请在crawly_addr2_url_bad中查看请求失败url&apos;)</div><div class="line">            break</div><div class="line">        elif status_code == 404:</div><div class="line">            # addr2_bad = &#123;&apos;url&apos;: result_url, &apos;status&apos;: 404&#125;</div><div class="line">            print(&apos;&#123;&#125;没有相关商户&apos;.format(result_url))</div><div class="line">            break</div><div class="line">        else:</div><div class="line">            get_msg_from(response, addr2_url)</div><div class="line"></div><div class="line"></div><div class="line">def requests_url(result_url, linktime=LINKTIME):</div><div class="line">    s = requests.Session()</div><div class="line">    headers = random.choice(USER_AGENT)</div><div class="line">    s.headers.update(&#123;&apos;User-Agent&apos;: headers&#125;)</div><div class="line">    proxies = &#123;&apos;http&apos;:random.choice(PROXY)&#125;</div><div class="line">    try:</div><div class="line">        r = s.get(result_url, proxies=proxies, timeout=TIMEOUT)</div><div class="line">        time.sleep(1)</div><div class="line">        return (r.status_code, r)</div><div class="line">    except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout,</div><div class="line">           requests.exceptions.ConnectionError):</div><div class="line">        if linktime &gt; 0:</div><div class="line">            print(&apos;请求失败，现在重新链接&apos;)</div><div class="line">            linktime -= 1</div><div class="line">            requests_url(result_url, linktime)</div><div class="line">        else:</div><div class="line">            status_code = &apos;link_bad&apos;</div><div class="line">            response = None</div><div class="line">            return (status_code, response)</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    addr2_url = &apos;http://www.dianping.com/search/category/219/10/g217r27028p3&apos;</div></pre></td></tr></table></figure></p>
<ol>
<li><code>requests_url</code>函数用于请求网页，并返回状态码和response。</li>
<li><code>get_msg_from</code>函数用于解析源代码，定位元素，爬取并存储所有我们需要的商户信息。</li>
<li><code>get_all_msg_from</code>函数实现了多页码爬取，当遇到网页状态码为<code>404</code>时，代表此时没有相关商户，最后一页已经被爬取，自动跳出for循环。结束某个addr2_url的爬取。</li>
</ol>
<p><code>run.py</code>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"># coding=utf-8</div><div class="line"></div><div class="line">from parsing.cate_parsing import GetTagAddr</div><div class="line">from parsing.shop_parsing import get_all_msg_from, crawly_addr2_ok</div><div class="line">from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    start_url = &apos;http://www.dianping.com/search/category/219/10/g0r0&apos;</div><div class="line">    tag_addr_task = GetTagAddr()</div><div class="line"></div><div class="line">    # 爬取tag1_url</div><div class="line">    tag_addr_task.get_tag1_from(start_url)</div><div class="line">    print(&apos;tag1_url爬取完成&apos;)</div><div class="line"></div><div class="line">    # 爬取tag2_url</div><div class="line">    tag1_wait = set(i[&apos;url&apos;] for i in tag_addr_task.tag1_url_db.find())</div><div class="line">    tag1_ok = set(i[&apos;url&apos;] for i in tag_addr_task.crawly_tag1_ok.find())</div><div class="line">    tag1_task = tag1_wait - tag1_ok</div><div class="line">    for tag1_url in tag1_task:</div><div class="line">        tag_addr_task.get_tag2_from(tag1_url)</div><div class="line">    print(&apos;tag2_url爬取完成&apos;)</div><div class="line"></div><div class="line">    # 爬取addr1_url</div><div class="line">    tag2_wait = set(i[&apos;url&apos;] for i in tag_addr_task.tag2_url_db.find())</div><div class="line">    tag2_ok = set(i[&apos;url&apos;] for i in tag_addr_task.crawly_tag2_ok.find())</div><div class="line">    tag2_task = tag2_wait - tag2_ok</div><div class="line">    for tag2_url in tag2_task:</div><div class="line">        tag_addr_task.get_addr1_from(tag2_url)</div><div class="line">    print(&apos;addr1_url爬取完成&apos;)</div><div class="line"></div><div class="line">    # 爬取addr2_url</div><div class="line">    addr1_wait = set(i[&apos;url&apos;] for i in tag_addr_task.addr1_url_db.find())</div><div class="line">    addr1_ok = set(i[&apos;url&apos;] for i in tag_addr_task.crawly_addr1_ok.find())</div><div class="line">    addr1_task = addr1_wait - addr1_ok</div><div class="line">    with ProcessPoolExecutor(max_workers=4) as executor:</div><div class="line">        executor.map(tag_addr_task.get_addr2_from, addr1_task)</div><div class="line">    print(&apos;addr2_url爬取完成&apos;)</div><div class="line"></div><div class="line">    # 根据addr2_url爬取商户信息</div><div class="line">    addr2_wait = set(i[&apos;url&apos;] for i in tag_addr_task.addr2_url_db.find())</div><div class="line">    addr2_ok = set(i[&apos;url&apos;] for i in crawly_addr2_ok.find())</div><div class="line">    addr2_task = addr2_wait - addr2_ok</div><div class="line">    with ThreadPoolExecutor(max_workers=8) as executor:</div><div class="line">        for url in addr2_task:</div><div class="line">            v = executor.submit(get_all_msg_from, url)</div><div class="line">        executor.shutdown(wait=True)</div><div class="line">            # executor.map(get_all_msg_from, addr2_task, chunksize=50)</div><div class="line">    print(&apos;addr2_url_reslut_url爬取完成&apos;)</div></pre></td></tr></table></figure></p>
<ol>
<li>爬取<code>addr2_url</code>采用多进程爬取，多进程通过<code>concurrent.futures.ProcessPoolExecutor</code>实现。</li>
<li>爬取<code>dpshop_msg</code>采用了多线程爬取，多线程通过<code>congurrent.futures.ThreadPoolExecutor</code>实现。</li>
</ol>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2017-11-04T14:29:19.000Z" itemprop="dateUpdated">2017-11-04 22:29:19</time>
</span><br>


        
        文章链接<a href="/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/" target="_blank" rel="external">https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/</a>
        
    </div>
    <footer>
        <a href="https://richardrw.github.io">
            <img src="/img/author.jpg" alt="Richard">
            Richard
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python爬虫/">Python爬虫</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/多进程/">多进程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/大众点评/">大众点评</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/&title=《构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）》 — Richard‘s Blog&pic=https://richardrw.github.io/img/author.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/&title=《构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）》 — Richard‘s Blog&source=the day with Python 爱生活，爱体验，工作认真，生活随意" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）》 — Richard‘s Blog&url=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/&via=https://richardrw.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/10/20/从美食的角度看东莞（数据来源：大众点评）/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">从美食的角度看东莞（数据来源：大众点评）</h4>
      </a>
    </div>
  
</nav>



    


<section class="comments" id="comments">
    <div id="disqus_thread"></div>
    <script>
    var disqus_shortname = 'true';
    lazyScripts.push('//' + disqus_shortname + '.disqus.com/embed.js')
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>













</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        支持作者写出更好的文章
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/zhifubao.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Richard &copy; 2015 - 2017</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/&title=《构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）》 — Richard‘s Blog&pic=https://richardrw.github.io/img/author.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/&title=《构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）》 — Richard‘s Blog&source=the day with Python 爱生活，爱体验，工作认真，生活随意" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）》 — Richard‘s Blog&url=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/&via=https://richardrw.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://richardrw.github.io/2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADMklEQVR42u3awW6jUAwF0P7/T6fSrCqlIdc2UwVzWEWE8jhk4b5rf33Fx+Pf8fz555nn86+uf3XN832O183v3zywsbGxL8J+HB5V6vEdjs8k5/NVXl2DjY2NvZXdK1rnvpRqwRsVOWxsbOxbso8XywtSUsbyzQY2NjY2drWAVTcVyZnqRggbGxsbu7dwr9gk11df2Z9madjY2Ngfz540ej/t8x/1t7GxsbE/hv0YHHlp6RXCPFoqPzk2Njb2InYe0OeL9R6xuglJtjRvRNjY2Ngr2PlDnNWIrQZJPWqzbmNjY2Nflj1Z+ISHaL2s6sbjl00INjY29gp2r007aQwnd5sUufz+2NjY2DvYk4c4JiWAXoGc/xjY2NjYm9hJZZs0fautgupWJ3/dv9wfGxsbex07j4eSEpVvKibhUbXgRYkaNjY29qXYPUyyqehF//PNSbJdwcbGxr4Du3CLwWDNcbnKo6I8DsPGxsbex55nLPnf9tq31Tip3N/GxsbGXsrOI/h5S6A3snP8PIUDGxsb++LsanGqYpJve03fSVyFjY2NvYmdjLzk5ac3BtTb9lTHgN70t7GxsbEvy87bAJNwvxdITSKnNz8PNjY29iJ2bzQn+Xb+4qrPUI6osLGxsRex85gpD5iqyXx13UlrARsbG3sTO4/je8MxvYZBrwgVGhLY2NjYi9h56zSPdXrRUrVlW42ZysM92NjY2BdhV0tCb+6l14SYDOu8+YyNjY29jt1rEpw1gjMpWs1yiI2Njb2ane9gqk3Z/GVNgqEoosLGxsZexJ40dM+K8vMzvSuxsbGx78CuxjTVxXqRU370Yi9sbGzs+7Crw5R5SyApNr3RzJ4FGxsb++rsvB2bB0+9rUu1LPUKMDY2NvYOdu+f/oRRaLu2IqSoAXDcFcHGxsZewc6PfPnektWWc/635R0YNjY29gXZ1fA9581HbfK2cTXGwsbGxt7H7jV6J7DJkE01VHoZLWFjY2Pfkv0/Grr5K8tbv29WwcbGxr4lO2n9VjcnedA/KVfY2NjYd2BPikT1FfRGdnrfRmOX2NjY2BdnV8vDvLRUS86kdXFafxsbGxv7E9nfUgiglG7HNtwAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>






<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '前往理查德生活记录';
            clearTimeout(titleTime);
        } else {
            document.title = '理查德生活记录';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
