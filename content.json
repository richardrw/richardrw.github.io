[{"title":"构建爬取大众点评美食数据的多线程爬虫（含多进程实现）（一）","date":"2017-10-20T14:07:02.000Z","path":"2017/10/20/构建爬取大众点评美食数据的多进程爬虫（一）/","text":"构建爬取大众点评美食数据的多进程爬虫（一）一、要抓取哪些数据以东莞美食为例大众点评-美食-东莞，可以看到，在美食页面，我们想要爬取的信息有： 商户标题 星级 评论数 人均价格 各项评分 菜系 地址 二、分析页面（一）多观察页面通过点击小吃快餐，我们看到小吃快餐下一共有50个页面。再点击面包甜点，发现也只有50个页面。接下来我们看看面包甜点-东城区，发现最多也只有50个页面。如此类推，我们大概可以确定，某菜系-某区域最多只有50个页面，由此基本可以判断，这应该是大众点评做大反爬虫措施。对此，我们在稍后的爬虫设计中，不能简单地从某一菜系下爬取所有页面或者从某一区域下爬取所有页面，因为单独选定菜系或者区域，服务器最多只返回50个页面给你。 因此我们需要将菜系-区域组合起来爬取，尽可能多的爬取商户信息。 通过观察，我们发现在一个网页中，如：http://www.dianping.com/search/category/219/10/g117r434g117代表菜系，r434代表区域，因此我们可以获取所有的菜系链接，接着在菜系链接的基础上获取菜系-区域链接，这就相当于在用浏览器浏览时，先选定了某一菜系，再选定某一区域。 （二）分析页面结构这里就是要通过定位来选定我们需要的元素。如一级菜系，它所处的位置是id=&quot;classfy的div标签下的a标签中。商户标题，它所处的位置是class=&quot;tit&quot;的div标签下的第一个a标签中。通过一个个地查看，我们可以得出我们要爬取的7个商户信息的位置，方便我们后续设计爬虫时进行定位。 三、设计爬虫（一）爬虫思路 爬虫流程图 从start_url开始，爬取所有一级菜系链接，得到tag1_url，存入数据库。 从数据库中读取tag1_url，爬取所有二级菜系链接，得到tag2_url，存入数据库。 从数据库中读取tag2_url，爬取所有一级区域链接，得到addr1_url，存入数据库。 从数据库中读取addr1_url，爬取所有二级区域链接，得到addr2_url，存入数据库。 从数据库中读取addr2_url，爬取所有商户信息，得到dpshop_msg，存入数据库。1-4目的都是一样，为了获取最终要爬取的页面链接，5就是为了实际爬取上述7个商户信息，所以我们把1-4写到cate_parsing.py文件中，把5写到shop_parsing.py文件中。另外，为了应对反爬虫，我们将用于伪装的User-Agent和代理IP等一些爬虫参数写到config.py文件中。（二）构建代码config.py代码如下：1234567891011121314151617181920212223242526272829303132#coding=utf-8#伪装浏览器USER_AGENT = [ &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36&apos;, &apos;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11&apos;, &apos;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0&apos;, &apos;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&apos; ]# 代理IPPROXY = [ &apos;111.13.111.184:80&apos;, &apos;49.119.164.175:80&apos;, &apos;61.136.163.245:3128&apos;, &apos;116.199.2.210:80&apos;, &apos;116.199.2.209:80&apos;, &apos;116.199.115.79:80&apos;, &apos;116.199.2.196:80&apos;, &apos;121.40.199.105:80&apos;, &apos;125.77.25.118:80&apos;, &apos;122.228.253.55:808&apos; ]TIMEOUT = 5LINKTIME = 3PAGE_NUM_MAX = 50 cate_parsing.py代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141#coding=utf-8import requestsimport randomimport pymongoimport timefrom lxml import etreefrom config import USER_AGENT, PROXY, TIMEOUT, LINKTIMEclient = pymongo.MongoClient(&apos;localhost&apos;, 27017)dp = client[&apos;dp&apos;]# 爬取一级菜系、二级菜系、二级菜系下的一级地址class GetTagAddr(object): headers = random.choice(USER_AGENT) proxies = &#123;&apos;http&apos;:random.choice(PROXY)&#125; s = requests.Session() s.headers.update(&#123;&apos;User-Agent&apos;:headers&#125;) linktime = LINKTIME timeout = TIMEOUT tag1_url_db = dp[&apos;tag1_url_db&apos;] # 存储从start_url中成功爬取到的tag1_url tag2_url_db = dp[&apos;tag2_url_db&apos;] # 存储从tag1_url中成功爬取到的tag2_url crawly_tag1_ok = dp[&apos;crawly_tag1_ok&apos;] # 存储爬取成功的tag1_url addr1_url_db = dp[&apos;addr1_url_db&apos;] # 存储从tag2_url中成功爬取到的addr1_url crawly_tag2_ok = dp[&apos;crawly_tag2_ok&apos;] # 存储爬取成功的tag2_url addr2_url_db = dp[&apos;addr2_url_db&apos;] # 存储从addr1_url中成功爬取到的addr2_url crawly_addr1_ok = dp[&apos;crawly_addr1_ok&apos;] # 存储爬取成功的addr1_url def get_tag1_from(self, start_url): try: r = self.s.get(start_url, proxies=self.proxies, timeout=self.timeout) tree = etree.HTML(r.text) tag1_items = tree.xpath(&apos;//div[@id=&quot;classfy&quot;]/a&apos;) for i in tag1_items: tag1 = i.getchildren()[0].text url = i.attrib[&apos;href&apos;] tag1_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;url&apos;: url, &apos;status&apos;: &apos;ok&apos;&#125; self.tag1_url_db.insert_one(tag1_url_msg) self.linktime = 3 # 重置linktime time.sleep(1) except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError): if self.linktime &gt; 0: print(&apos;爬取失败，现在重新链接&apos;) self.get_tag1_from(start_url) self.linktime -= 1 else: print(&apos;&#123;&#125;爬取失败&apos;.format(start_url)) def get_tag2_from(self, tag1_url): tag1_url_msg = self.tag1_url_db.find_one(&#123;&apos;url&apos;: tag1_url&#125;) tag1 = tag1_url_msg[&apos;tag1&apos;] try: r = self.s.get(tag1_url, proxies=self.proxies, timeout=self.timeout) tree = etree.HTML(r.text) tag2_items = tree.xpath(&apos;//div[@id=&quot;classfy-sub&quot;]/a&apos;) if len(tag2_items) == 0: tag2 = &apos;no_sub&apos; tag2_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;url&apos;: tag1_url, &apos;status&apos;: &apos;tag1_not_sub&apos;&#125; self.tag2_url_db.insert_one(tag2_url_msg) else: for i in tag2_items: tag2 = i.getchildren()[0].text url = i.attrib[&apos;href&apos;] tag2_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;url&apos;: url, &apos;status&apos;: &apos;ok&apos;&#125; self.tag2_url_db.insert_one(tag2_url_msg) self.crawly_tag1_ok.insert_one(&#123;&apos;url&apos;: tag1_url&#125;) self.linktime = 3 # 重置linktime time.sleep(1) except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError): if self.linktime &gt; 0: print(&apos;爬取失败，现在重新链接&apos;) self.get_tag2_from(tag1_url) self.linktime -= 1 else: print(&apos;&#123;&#125;爬取失败&apos;.format(tag1_url)) def get_addr1_from(self, tag2_url): tag2_url_msg = self.tag2_url_db.find_one(&#123;&apos;url&apos;: tag2_url&#125;) tag1 = tag2_url_msg[&apos;tag1&apos;] tag2 = tag2_url_msg[&apos;tag2&apos;] try: r = self.s.get(tag2_url, proxies=self.proxies, timeout=self.timeout) tree = etree.HTML(r.text) addr1_items = tree.xpath(&apos;//div[@id=&quot;region-nav&quot;]/a&apos;) for i in addr1_items: addr1 = i.getchildren()[0].text url = i.attrib[&apos;href&apos;] addr1_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;: addr1, &apos;url&apos;: url&#125; self.addr1_url_db.insert_one(addr1_url_msg) self.crawly_tag2_ok.insert_one(&#123;&apos;url&apos;: tag2_url&#125;) self.linktime = 3 # 重置linktime time.sleep(1) except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError): if self.linktime &gt; 0: print(&apos;爬取失败，现在重新链接&apos;) self.get_addr1_from(tag2_url) self.linktime -= 1 else: print(&apos;&#123;&#125;爬取失败&apos;.format(tag2_url)) def get_addr2_from(self, addr1_url): addr1_url_msg = self.addr1_url_db.find_one(&#123;&apos;url&apos;: addr1_url&#125;) tag1 = addr1_url_msg[&apos;tag1&apos;] tag2 = addr1_url_msg[&apos;tag2&apos;] addr1 = addr1_url_msg[&apos;addr1&apos;] try: r = self.s.get(addr1_url, proxies=self.proxies, timeout=self.timeout) tree = etree.HTML(r.text) addr2_items = tree.xpath(&apos;//div[@id=&quot;region-nav-sub&quot;]/a&apos;) if len(addr2_items) == 0: addr2 = &apos;no_sub&apos; addr2_url_msg = &#123;&apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;: addr1, &apos;addr2&apos;: addr2, &apos;url&apos;: addr1_url, &apos;status&apos;: &apos;addr1_not_sub&apos;&#125; self.addr2_url_db.insert_one(addr2_url_msg) else: for i in addr2_items: addr2 = i.getchildren()[0].text url = i.attrib[&apos;href&apos;] addr2_url_msg = &#123;&apos;tag1&apos;:tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;:addr1, &apos;addr2&apos;: addr2, &apos;url&apos;:url&#125; self.addr2_url_db.insert_one(addr2_url_msg) self.crawly_addr1_ok.insert_one(&#123;&apos;url&apos;: addr1_url&#125;) self.linktime = 3 # 重置linktime time.sleep(1) except( requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError): if self.linktime &gt; 0: print(&apos;爬取失败，现在重新链接&apos;) self.get_addr2_from(addr1_url) self.linktime -= 1 else: print(&apos;&#123;&#125;爬取失败&apos;.format(addr1_url))if __name__ == &apos;__main__&apos;: url = &apos;http://www.dianping.com/search/category/219/10/g0r0&apos; get_tag1_from, get_tag2_from, get_addr1_from和get_addr2_from都采用了同样的逻辑： 先请求url，获得response。 使用etree.HTML解析网页。 定位所要爬取元素的位置。 存储。 shop_parsing.py代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#coding=utf-8import requestsimport randomimport pymongoimport timefrom lxml import etreefrom config import USER_AGENT, PROXY, TIMEOUT, LINKTIME, PAGE_NUM_MAXclient = pymongo.MongoClient(&apos;localhost&apos;, 27017)dp = client[&apos;dp&apos;]addr2_url_db = dp[&apos;addr2_url_db&apos;] # 存储从addr1_url中成功爬取到的addr2_urldpshop = dp[&apos;dpshop&apos;] # 存储从addr2中成功爬取到的dpshop_msgcrawly_addr2_ok = dp[&apos;crawly_addr2_ok&apos;] # 存储爬取成功的addr2_url# def callback(future):# response, addr2_url = future.result()# get_msg_from(response, addr2_url)def get_msg_from(response, addr2_url): addr2_url_msg = addr2_url_db.find_one(&#123;&apos;url&apos;: addr2_url&#125;) tag1 = addr2_url_msg[&apos;tag1&apos;] tag2 = addr2_url_msg[&apos;tag2&apos;] addr1 = addr2_url_msg[&apos;addr1&apos;] addr2 = addr2_url_msg[&apos;addr2&apos;] tree = etree.HTML(response.text) # 提取标题信息 title_items = tree.xpath(&apos;//div[@id=&quot;shop-all-list&quot;]//div[@class=&quot;tit&quot;]/a[1]&apos;) # 提取商户星级 star_items = tree.xpath(&apos;//div[@class=&quot;comment&quot;]/span&apos;) # 提取评论数-人均价格 review_price_items = tree.xpath(&apos;//div[@class=&quot;comment&quot;]/a&apos;) review_price_list = [] for i in review_price_items: # 判断评论数-价格的a标签是否存在子元素，如果存在，则在子元素中提取评论数-价格，否则评论数-价格都为“None” if i.getchildren(): text = i.getchildren()[0].text review_price_list.append(text) else: review_price_list.append(&quot;None&quot;) review_price_list = [review_price_list[i:i + 2] for i in range(0, len(review_price_list), 2)] # 提取各项评分 score_items = tree.xpath(&apos;//span[@class=&quot;comment-list&quot;]/span/b&apos;) score_items = [i.text for i in score_items] score_items = [score_items[i:i + 3] for i in range(0, len(score_items), 3)] # 提取菜系-商区-详细地址 tag_items = tree.xpath(&apos;//div[@class=&quot;tag-addr&quot;]//span&apos;) tag_items = [i.text for i in tag_items] tag_items = [tag_items[i:i + 3] for i in range(0, len(tag_items), 3)] for title_url, star, review_price, score, tag_area_addr in zip(title_items, star_items, review_price_list, score_items, tag_items): dpshop_msg = &#123; &apos;title&apos;: title_url.attrib[&apos;title&apos;], &apos;url&apos;: title_url.attrib[&apos;href&apos;], &apos;star&apos;: star.attrib[&apos;title&apos;], &apos;review&apos;: review_price[0], &apos;price&apos;: review_price[1], &apos;taste&apos;: score[0], &apos;env&apos;: score[1], &apos;service&apos;: score[2], &apos;tag1&apos;: tag1, &apos;tag2&apos;: tag2, &apos;addr1&apos;: addr1, &apos;addr2&apos;: addr2, &apos;full_addr&apos;: tag_area_addr[2] &#125; dpshop.insert_one(dpshop_msg) crawly_addr2_ok.insert_one(&#123;&apos;url&apos;: addr2_url&#125;)def get_all_msg_from(addr2_url): for page in range(1, PAGE_NUM_MAX+1): result_url = &apos;&#123;&#125;p&#123;&#125;&apos;.format(addr2_url, page) status_code, response = requests_url(result_url) if status_code == &apos;link_bad&apos;: print(&apos;请求失败，请在crawly_addr2_url_bad中查看请求失败url&apos;) break elif status_code == 404: # addr2_bad = &#123;&apos;url&apos;: result_url, &apos;status&apos;: 404&#125; print(&apos;&#123;&#125;没有相关商户&apos;.format(result_url)) break else: get_msg_from(response, addr2_url)def requests_url(result_url, linktime=LINKTIME): s = requests.Session() headers = random.choice(USER_AGENT) s.headers.update(&#123;&apos;User-Agent&apos;: headers&#125;) proxies = &#123;&apos;http&apos;:random.choice(PROXY)&#125; try: r = s.get(result_url, proxies=proxies, timeout=TIMEOUT) time.sleep(1) return (r.status_code, r) except(requests.exceptions.ProxyError, requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError): if linktime &gt; 0: print(&apos;请求失败，现在重新链接&apos;) linktime -= 1 requests_url(result_url, linktime) else: status_code = &apos;link_bad&apos; response = None return (status_code, response)if __name__ == &apos;__main__&apos;: addr2_url = &apos;http://www.dianping.com/search/category/219/10/g217r27028p3&apos; requests_url函数用于请求网页，并返回状态码和response。 get_msg_from函数用于解析源代码，定位元素，爬取并存储所有我们需要的商户信息。 get_all_msg_from函数实现了多页码爬取，当遇到网页状态码为404时，代表此时没有相关商户，最后一页已经被爬取，自动跳出for循环。结束某个addr2_url的爬取。 run.py代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# coding=utf-8from parsing.cate_parsing import GetTagAddrfrom parsing.shop_parsing import get_all_msg_from, crawly_addr2_okfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutorif __name__ == &apos;__main__&apos;: start_url = &apos;http://www.dianping.com/search/category/219/10/g0r0&apos; tag_addr_task = GetTagAddr() # 爬取tag1_url tag_addr_task.get_tag1_from(start_url) print(&apos;tag1_url爬取完成&apos;) # 爬取tag2_url tag1_wait = set(i[&apos;url&apos;] for i in tag_addr_task.tag1_url_db.find()) tag1_ok = set(i[&apos;url&apos;] for i in tag_addr_task.crawly_tag1_ok.find()) tag1_task = tag1_wait - tag1_ok for tag1_url in tag1_task: tag_addr_task.get_tag2_from(tag1_url) print(&apos;tag2_url爬取完成&apos;) # 爬取addr1_url tag2_wait = set(i[&apos;url&apos;] for i in tag_addr_task.tag2_url_db.find()) tag2_ok = set(i[&apos;url&apos;] for i in tag_addr_task.crawly_tag2_ok.find()) tag2_task = tag2_wait - tag2_ok for tag2_url in tag2_task: tag_addr_task.get_addr1_from(tag2_url) print(&apos;addr1_url爬取完成&apos;) # 爬取addr2_url addr1_wait = set(i[&apos;url&apos;] for i in tag_addr_task.addr1_url_db.find()) addr1_ok = set(i[&apos;url&apos;] for i in tag_addr_task.crawly_addr1_ok.find()) addr1_task = addr1_wait - addr1_ok with ProcessPoolExecutor(max_workers=4) as executor: executor.map(tag_addr_task.get_addr2_from, addr1_task) print(&apos;addr2_url爬取完成&apos;) # 根据addr2_url爬取商户信息 addr2_wait = set(i[&apos;url&apos;] for i in tag_addr_task.addr2_url_db.find()) addr2_ok = set(i[&apos;url&apos;] for i in crawly_addr2_ok.find()) addr2_task = addr2_wait - addr2_ok with ThreadPoolExecutor(max_workers=8) as executor: for url in addr2_task: v = executor.submit(get_all_msg_from, url) executor.shutdown(wait=True) # executor.map(get_all_msg_from, addr2_task, chunksize=50) print(&apos;addr2_url_reslut_url爬取完成&apos;) 爬取addr2_url采用多进程爬取，多进程通过concurrent.futures.ProcessPoolExecutor实现。 爬取dpshop_msg采用了多线程爬取，多线程通过congurrent.futures.ThreadPoolExecutor实现。","tags":[{"name":"Python爬虫","slug":"Python爬虫","permalink":"https://richardrw.github.io/tags/Python爬虫/"},{"name":"多进程","slug":"多进程","permalink":"https://richardrw.github.io/tags/多进程/"},{"name":"大众点评","slug":"大众点评","permalink":"https://richardrw.github.io/tags/大众点评/"}]},{"title":"从美食的角度看东莞（数据来源：大众点评）","date":"2017-10-20T13:35:45.000Z","path":"2017/10/20/从美食的角度看东莞（数据来源：大众点评）/","text":"从美食的角度看东莞（数据来源：大众点评）先从为什么要做这个项目说起，事情的缘由是这样的，小编当时跟领导到别的城市外出出差一个星期，作为一个平时很少外出的人，别说是别的城市了，就连东莞这个地方我都还没走遍过，尤其是那些重要商区、美食店，平时跟朋友出去玩，都很尴尬不知道要吃什么好。现在跟领导外出别的城市，感觉自己是个刚出大山的农村小伙子，对外面的世界所知甚少。作为领导的助手，自然就要安排好这一星期的就餐地点。然而这就很难为我了，我连东莞有什么地方有好吃的都不知道，现在居然要我在别的城市安排一个星期的就餐作息表？这就很尴尬了。于是我每天都苦于到大众点评、美团等APP上搜索附近美食，但是还是不知道那些餐店的真实情况怎样，环境如何，味道如何。于是此时我就想，如果能一下子就知道某个城市或者某个区中最受欢迎的店有哪些，哪个镇或者哪个区最繁华，那该多好。于是小编回来后就开始构思，接着就撸起袖子加油干，爬取了大众点评上1.5w多条数据，并做了个东莞美食商户数量热力图，从美食的角度看东莞。 东莞各镇街美食商户数量分布热力图 （因为图片大小问题，莞城、洪梅和大岭山的标签没有显示出来，要解决这个问题，可以找张大点的图制作SVG） 东莞各镇街美食商户数量分布热力图-动态 东莞各镇街美食商户数量柱状图 可以看出，东城区商户数量最多，其次是南城区，从热力图来看，也可以看出东莞哪个镇街最繁华，也可以初步判断出哪些镇街经济情况比较好。因为一般经济越好的地区，其商业越繁华越发达。为了验证这个猜想，小编查了下东莞2016年各镇街经济实力排名情况 东莞各镇街2017年经济实力排名情况 可以看出，经济实力与地区商户数量基本成正相关。这里说一个特例吧，那就是松山湖。本地人可能知道原因，那就是因为松山湖建立历史比较年轻，区域布局比较先进，目前仅有万科（好像是）这个大型商场，其他就以大学、高新技术产业区以及青山绿水为主，没有一脑子就开发许多商业区，所以就出现了这个经济实力与地区美食商户数量不匹配的情况。接下来看看各星级商户占比情况如何 东莞不同星级商户占比饼状图 准四星用户占73.98%，看来东莞的饮食服务业还是可以的（毕竟是广东啊哈哈）再看看东莞各菜系商户数量情况如何 东莞各菜系商户数量柱状图 小吃快餐最多，其次是面包甜点。小吃快餐最多的原因小编猜测可能有三： 大多数公司中的员工中午都在公司叫外卖，外卖以小吃快餐为主，需求较大，由此催生出一批小吃快餐店，以满足市场需求。 开小吃快餐店的成本比较低，像沙县小吃什么的快餐店，随便会做个番茄炒蛋饭青瓜肉片饭就能开个快餐店了（哈哈），比面包甜点、西餐的开店成本低。 大多数小吃快餐店都选择加入大众点评平台，以提高销售量，增加利润。 至于面包甜点有3155家，这让小编感到意外，没想到东莞居然有这么多面包店。好了，暂时就分析到这里，小编最爱吃的就是面包了，赶紧去看看里面有哪些面包店的，一定要去尝尝。","tags":[{"name":"数据分析","slug":"数据分析","permalink":"https://richardrw.github.io/tags/数据分析/"},{"name":"美食","slug":"美食","permalink":"https://richardrw.github.io/tags/美食/"},{"name":"东莞","slug":"东莞","permalink":"https://richardrw.github.io/tags/东莞/"}]},{"title":"年轻人是否到北上广深发展的博弈论分析","date":"2017-09-25T14:05:11.000Z","path":"2017/09/25/年轻人是否应该到北上广深发展的博弈论分析/","text":"一、思考背景有人说，当你在一个岗位上工作时间超过1年的时候，你就会不自觉地思考，思考诸如你当前工作的意义是什么、这个岗位是否真的适合你等这些“终极问题”。博主现在已经毕业一年零三个月，实际工作时间已有一年零九个月，在这时间点上，大脑变的不自主地思考起一些关于人生的是非对错问题。引发博主思考这个问题的背景主要有三： 曾为大学毕业生，也曾思考过到北上广深发展还是留守本地，驻地生根的问题。 身边有人说，到北上广深发展，只能当炮灰。在沉重的生活成本下，即使工资比二三线城市高，但是实际生活质量却不比二三线城市好，甚至更差。去了只能压榨自己，使自己成为一线城市的“蜡烛”，照亮了别人（成就了上层人士），燃烧了自己（折旧了自己）。 身边也有人说，到北上广深发展，机会更大。留在本地（二三线城市），固然稳定，生活压力也没那么大，但是一辈子都不会有什么变数，一生只能按现在的状态生活下去，极难遇到让自己人生变得更加美好的变数（机遇）。到底谁说的对，谁说的错，是该往北上广深发展，还是驻地生根。不同的人在做这个决定的时候，他考量的是什么，决策依据是什么，决策背后的理论根据又是什么。此时慢慢地、慢慢地，博主想通过博弈论的角度，看能不能分析出什么，想知道，思考的结果又能不能为博主指明前往更美好生活的方向。 写着写着发现，这题目太过严肃认真了，提不起兴趣来写，姑且就先来吹吹水吧。想必大家如果留意生活想象的话，应该不难发现这些现象：在足球比赛中，说说国足吧，一开始总是让人觉得放不开来打，等到落后比分的时候才奋力直追放开来打。在帆船比赛中，落后的选手总是不会老老实实地跟着领航者的路线，而是另谋航线以求超越。在LOL等电子竞技赛中，劣势的一方总是希望通过大龙来抢夺来争取一线赢机。为什么人们在处于劣势环境的情况下，总是追求那些很不稳定的方法呢？例如在篮球足球比赛中，落后一方总是采取多冒险的策略，去搞小动作去犯规。下面我们就来用数学先分析一下。我们假设在一场篮球比赛中，比赛的结果由比赛双方的能力和运气一起决定，你的取胜条件需满足： 你的能力 + 你的运气 &gt; 对手的能力 + 对手的运气 或者 你的运气 - 对手的运气 &gt; 你的能力 - 对手的能力 L &gt; S 在一场比赛中，你的能力、对手的能力基本是已经确定的，因为你不会像《龙珠》里的赛亚人那样，打着打着就变身为超级赛亚人提升战斗力，因此我们不妨将 “你的能力 - 对手的能力” 的差值S视为一确定的系数。那么剩下来，影响比赛结果的就是 “你的运气 - 对手的运气“ 的差值L能否大于S 。 假设对手的能力比你强，你处于劣势，此时S&gt;0，为正值。你要获胜，就必须使得“你的运气 - 对手的运气”的差值L大于S，即当且仅当L&gt;S，你才能获胜。那么我们要如何使得L&gt;S呢？运气这种东西，是我们能够控制的吗？ 我们不妨从数学的角度来看“你的运气 - 对手的运气”的差值L。因为你的运气和对手的运气都是不确定的，必然，“你的运气 - 对手的运气”的差值L也是不确定的，即L的值存在概率性，因此不妨假设L的概率分布是正态曲线（如下图中的实线曲线）。横轴x的任意点，代表L可能的取值，其到曲线上的高y值代表L取某值时对应的概率，因此曲线在两点间覆盖的面积等于L取值位于这两点间的概率。 L取值概率分布图 由图可见，你取胜的概率为图中网格的阴影面积，即对于所有L可能的取值，满足L&gt;S的取值的概率之和。到这里，我们已经知道“你的运气 - 对手的运气”的数学上的意义了。那么回到我们之前的问题：运气这种东西，是我们能够控制的吗？ 答案是可以的。或许从感性理解上，这会很不可思议，但是从数学的角度看，改变运气，就是让你改变“你的运气”的概率分布，使得“你的运气 - 对手的运气”大于S变得更加可能。那么我们需要怎么做呢？还是先从数学角度看。让“你的运气 - 对手的运气”的值大于S变得更加可能，也就是让你增大图中的网格阴影面积。 问：怎么增大？答：让正态曲线更加平坦即可。（如变成上图中的阴线曲线） 曲线变得更平坦后，由图可见，L&gt;S的面积增加了斜线阴影部分，也就是说L&gt;S的概率更大了。在这种情况下，你获胜的概率也就提升了。 曲线变得更平坦，意味着L的取值更具不确定性。在实际生活中，你一般只需采取更冒险、更激进的策略就能达到让“你的运气”的概率分布曲线变得更加平台的效果。就如篮球比赛中，对方实力比你强，你处于落后状态，这时，如果你陈规蹈矩，按部就班，那么你是很难有反超的机会的，因为你只能依靠进攻得分，而那些博犯规罚球得分等“非正常”得分你就很少有机会碰到，因为你打得很稳。相反，如果你采取冒险、更加激进的策略，你活得博犯规罚球得分的机会就会变大，得分机会变大，也就意味着你反超的机会变大。 因此，如果你面对一个比你强大的对手，而你又处于劣势状态，那么你就需要采取一些冒险的策略来打破这种“对手比你厉害，你处于劣势“的稳定状态，从而提高你的胜率。 回到标题中的问题，答案可以从上述中得出总结，只不过在这个比赛中，对手可能不是别人，而可能是你自己，或者整个大环境。你此时需要衡量一下自己的处境，自己能力对于大环境来说，是强是弱，自己的处境是劣势还是优势，在根据综合判断，看是否需要采取一些冒险的策略来打破平衡、打破稳定，提高自己的胜率。","tags":[{"name":"博弈论","slug":"博弈论","permalink":"https://richardrw.github.io/tags/博弈论/"},{"name":"稳定性","slug":"稳定性","permalink":"https://richardrw.github.io/tags/稳定性/"},{"name":"风险","slug":"风险","permalink":"https://richardrw.github.io/tags/风险/"},{"name":"均衡","slug":"均衡","permalink":"https://richardrw.github.io/tags/均衡/"},{"name":"北上广深","slug":"北上广深","permalink":"https://richardrw.github.io/tags/北上广深/"}]},{"title":"sublime如何添加python3版本以及中文乱码问题","date":"2017-09-23T16:05:05.000Z","path":"2017/09/24/sublime如何添加python3版本以及中文乱码问题/","text":"一、当同时安装了python2和python3时，如何让sublime text同时支持？ 在sublime text中选择Toos—&gt;Build System—&gt;New Build System，此时会创建一个新文件，内容如下： 123&#123; &quot;shell_cmd&quot;: &quot;make&quot;&#125; 将原有内容删除，把以下内容复制进去： 123456&#123; &quot;cmd&quot;: [&quot;/Library/Frameworks/Python.framework/Versions/3.6/bin/python3&quot;, &quot;-u&quot;, &quot;$file&quot;], &quot;file_regex&quot;: &quot;^[ ]*File \\&quot;(...*?)\\&quot;, line ([0-9]*)&quot;, &quot;selector&quot;: &quot;source.python&quot;, &quot;env&quot;: &#123;&quot;LANG&quot;: &quot;en_US.UTF-8&quot;&#125;&#125; 注意： /Library/Frameworks/Python.framework/Versions/3.6/bin/python3为你安装python3的路径 查看你python2或python3的路径方法。在终端／cmd下执行 which python获取python2路径，执行which python3获取python3路径，复制替换&quot;cmd&quot;中python的路径即可。 &quot;env&quot;: {&quot;LANG&quot;: &quot;en_US.UTF-8&quot;的作用是为了正常地显示中文 将文件保存为Python3.sublime-build，路径为sublime安装目录下的Packages文件夹 二、使用ConvertToUTF8解决中文乱码 使用Ctrl+Shift+P打开Package Control，输入install package按回车，再搜索ConvertToUTF8来安装插件 安装完后再次使用Ctrl+Shift+P打开Package Control，这次输入ConvertToUTF8，回车，再选择UTF-8编码即可。这样就会以utf-8的编码格式编辑文件。","tags":[{"name":"sublime","slug":"sublime","permalink":"https://richardrw.github.io/tags/sublime/"},{"name":"python","slug":"python","permalink":"https://richardrw.github.io/tags/python/"},{"name":"utf-8","slug":"utf-8","permalink":"https://richardrw.github.io/tags/utf-8/"}]},{"title":"test_my_site","date":"2017-09-16T07:23:04.000Z","path":"2017/09/16/test-my-site/","text":"","tags":[]},{"title":"Hello World","date":"2017-09-16T07:08:10.000Z","path":"2017/09/16/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]